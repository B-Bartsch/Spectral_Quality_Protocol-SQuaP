{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "P6y7KQgQGLJj",
        "xLLw6PipG5e0",
        "HVGp23KkpERT",
        "PaeMkL81u3c_",
        "43pW8aYm125B",
        "sBZiuz6M93QW",
        "_6shgha0ASFY",
        "oN9Nql3DCcXw",
        "XrMIUcKhMtev",
        "NPDBd2rnNDFQ",
        "Xw3BxVtr9SOq",
        "X2yf1A-ZswIM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1° - Convolution of hyperspectral data**"
      ],
      "metadata": {
        "id": "P6y7KQgQGLJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91fFrPQ2D5aX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/validation_CS/protocol_DB_validation\")\n",
        "\n",
        "# Reading the CSV file\n",
        "DB_raw = pd.read_csv(\"DB_validation_raw_clustered.csv\", encoding='ISO-8859-1')\n",
        "\n",
        "hyper_data = DB_raw\n",
        "\n",
        "# Select the ID column number and the numbers where the hyperspectral bands begin and end\n",
        "hyper_data = hyper_data.iloc[:, [0] + list(range(16, 2167))]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(hyper_data)"
      ],
      "metadata": {
        "id": "RmE0LkB0Ea5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################\n",
        "#  At this stage of the script, data from hyperspectral bands    #\n",
        "#  to multispectral bands is convolved.                          #\n",
        "#  Pay attention to the script comments at this stage            #\n",
        "##################################################################\n",
        "\n",
        "\n",
        "# Imports\n",
        "# Se estiver executando o script na máquina pessoal, insira o diretório da linguagem R instalada na máquina\n",
        "os.environ['R_HOME'] = 'C:/Program Files/R/R-4.3.2'\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr\n",
        "\n",
        "# Saving Dataframes to CSV\n",
        "\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/validation_CS/protocol_DB_validation\")\n",
        "hyper_data.to_csv(\"hyper_data.csv\", index=False)\n",
        "\n",
        "# R Script\n",
        "r_script = \"\"\"\n",
        "\n",
        "# Reading CSV files\n",
        "\n",
        "# Ajuste do limite de memória no início do script\n",
        "#memory.limit(size=20000000)\n",
        "\n",
        "setwd(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/validation_CS/protocol_DB_validation\")\n",
        "hyper_data <- read.csv(\"hyper_data.csv\", header = TRUE, sep = \",\")\n",
        "\n",
        "rownames(hyper_data) <- hyper_data[, 1]\n",
        "hyper_data1 <- hyper_data\n",
        "hyper_data1 <- hyper_data1[, -1]\n",
        "ID_Unique <- hyper_data[, 1]\n",
        "colnames(hyper_data1) <- seq(from = 350, to = 2500, by = 1)\n",
        "\n",
        "\n",
        "##############################################################################################\n",
        "## R: Directory path to the required hsdar package libraries##                               #\n",
        "#                                                                                            #\n",
        ".libPaths(\"G:/OneDrive/Documentos/Doutorado/protocol_database/libraires\")                    #\n",
        "#                                                                                            #\n",
        "##############################################################################################\n",
        "\n",
        "# Load the hsdar package\n",
        "library(hsdar)\n",
        "\n",
        "\n",
        "# Create a matrix of the data\n",
        "hyper_data.matrix <- as.matrix(hyper_data1)\n",
        "\n",
        "# Wavelengths for hyperspectral data\n",
        "wave.hyper_data <- seq(from = 350, to = 2500, by = 1)\n",
        "\n",
        "# Create speclib of the data\n",
        "hyper_data.speclib <- speclib(hyper_data.matrix, wave.hyper_data)\n",
        "\n",
        "# Get characteristics of the sensor\n",
        "get.sensor.characteristics(\"Sentinel2a\", response_function = TRUE)\n",
        "\n",
        "\n",
        "############################################################################################\n",
        "#The following command will obtain the channel wavelength of deployed\n",
        "#satellite sensors (multispectral).\n",
        "#Below is a list of sensor names available for operation.\n",
        "#Copy and paste the name into the command:\n",
        "\n",
        "#\"Landsat4\"\n",
        "#\"Landsat5\"\n",
        "#\"Landsat7\"\n",
        "#\"Landsat8\"\n",
        "#\"Sentinel2a\"\n",
        "#\"Sentinel2b\"\n",
        "#\"RapidEye\n",
        "#\"WorldView2-8\"\n",
        "#\"Quickbird\"\n",
        "#\"WorldView2-4\"\n",
        "\n",
        "# Perform spectral resampling\n",
        "multi_data.sentinel2.1 <- spectralResampling(hyper_data.speclib, \"Sentinel2a\",\n",
        "                                            response_function = TRUE)\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "# Plot a graph\n",
        "plot(multi_data.sentinel2.1[1])\n",
        "\n",
        "# Summary statistics\n",
        "summary(multi_data.sentinel2.1)\n",
        "\n",
        "\n",
        "# Round the values before transforming into a dataframe\n",
        "resampled_spectra <- as.data.frame(multi_data.sentinel2.1)\n",
        "\n",
        "#resampled_spectra <- round(as.data.frame(multi_data.sentinel2.1))\n",
        "\n",
        "# Add IDs back\n",
        "resampled_spectra <- cbind(ID_Unique, resampled_spectra)\n",
        "\n",
        "# Write the dataframe to a CSV file\n",
        "setwd(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/validation_CS/protocol_DB_validation\")\n",
        "write.csv(resampled_spectra, \"resampled_spectra.csv\", row.names = FALSE)\n",
        "\n",
        "\"\"\"\n",
        "# Executing the R script\n",
        "robjects.r(r_script)"
      ],
      "metadata": {
        "id": "Qerx7jx2Epo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/validation_CS/protocol_DB_validation\")\n",
        "# Reading the CSV file\n",
        "\n",
        "resampled_spectra = pd.read_csv(\"resampled_spectra.csv\")\n",
        "\n",
        "print(resampled_spectra)"
      ],
      "metadata": {
        "id": "BQudsF_1E9PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data handling for Landsat 8 spectral resolution**"
      ],
      "metadata": {
        "id": "QMV7jy3cq7PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Landsat 8 spectral resolution:\n",
        "\n",
        "# B1.........Costal...........430-450 (nm)\n",
        "# B2.........Blue.............450-510 (nm)\n",
        "# B3.........Green............530-590 (nm)\n",
        "# B4.........Red..............640-670 (nm)\n",
        "# B5.........NIR..............850-880 (nm)\n",
        "# B9.........Cirrus.........1360-1380 (nm)\n",
        "# B6.........SWIR1..........1570-1650 (nm)\n",
        "# B7.........SWIR2..........2110-2290 (nm)\n",
        "\n",
        "\n",
        "# Create a dictionary of correspondence between original and new names\n",
        "corresponding_name = {\n",
        "    \"ID_Unique\": \"ID_Unique\",\n",
        "    \"Costal\": \"Costal_Landsat8_resampled\",\n",
        "    \"Blue\": \"Blue_Landsat8_resampled\",\n",
        "    \"Green\": \"Green_Landsat8_resampled\",\n",
        "    \"Red\": \"Red_Landsat8_resampled\",\n",
        "    \"NIR\": \"NIR_Landsat8_resampled\",\n",
        "    \"Cirrus\": \"Cirrus_Landsat8_resampled\",\n",
        "    \"SWIR1\": \"SWIR1_Landsat8_resampled\",\n",
        "    \"SWIR2\": \"SWIR2_Landsat8_resampled\"\n",
        "}\n",
        "\n",
        "# Rename the columns of the resampled_spectra dataframe\n",
        "resampled_spectra.columns = [corresponding_name[col] for col in resampled_spectra.columns]\n",
        "\n",
        "# Merge the dataframes\n",
        "DB_raw_resampled_spectra = pd.merge(DB_raw, resampled_spectra, on=\"ID_Unique\")"
      ],
      "metadata": {
        "id": "Rw94Jn70i2pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DB_raw_resampled_spectra)"
      ],
      "metadata": {
        "id": "kB_Bk3zmrQdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Dataframes to CSV\n",
        "DB_raw_resampled_spectra.to_csv(\"DB_raw_resampled_spectra.csv\", index=False)"
      ],
      "metadata": {
        "id": "XPYQGRbxrRNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data handling for Sentinel-2A spectral resolution**"
      ],
      "metadata": {
        "id": "Ihy671sqrV2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentinel-2A spectral resolution:\n",
        "# B1.........Aerossol.........430-450 (nm)\n",
        "# B2.........Blue.............458-523 (nm)\n",
        "# B3.........Green............543-578 (nm)\n",
        "# B4.........Red..............650-680 (nm)\n",
        "# B5.........Red Edge 1.......698-713 (nm)\n",
        "# B6.........Red Edge 2.......733-748 (nm)\n",
        "# B7.........Red Edge 3.......773-793 (nm)\n",
        "# B8.........NIR..............785-899 (nm)\n",
        "# B8A........Red Edge 4.......855-875 (nm)\n",
        "# B9.........Water Vapor......930-950 (nm)\n",
        "# B10........Cirrus...........1360-1390 (nm)\n",
        "# B11........SWIR 1...........1565-1655 (nm)\n",
        "# B12........SWIR 2...........2100-2280 (nm)\n",
        "\n",
        "# Create a dictionary of correspondence between original and new names\n",
        "corresponding_name = {\n",
        "    \"ID_Unique\": \"ID_Unique\",\n",
        "    \"SR_AV_B1\": \"Aerosol_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B2\": \"Blue_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B3\": \"Green_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B4\": \"Red_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B5\": \"Red_Edge_1_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B6\": \"Red_Edge_2_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B7\": \"Red_Edge_3_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B8\": \"NIR_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B8A\": \"Red_Edge_4_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B9\": \"Water_Vapor_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B10\": \"Cirrus_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B11\": \"SWIR_1_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B12\": \"SWIR_2_Sentinel2A_resampled\"\n",
        "}\n",
        "\n",
        "# Rename the columns of the resampled_spectra dataframe\n",
        "resampled_spectra.columns = [corresponding_name[col] for col in resampled_spectra.columns]\n",
        "\n",
        "# Merge the dataframes\n",
        "DB_raw_resampled_spectra = pd.merge(DB_raw, resampled_spectra, on=\"ID_Unique\")"
      ],
      "metadata": {
        "id": "4DD7bYDcFMd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DB_raw_resampled_spectra)"
      ],
      "metadata": {
        "id": "kZADYvObFTxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Dataframes to CSV\n",
        "DB_raw_resampled_spectra.to_csv(\"DB_validation_raw_resampled_spectra.csv\", index=False)"
      ],
      "metadata": {
        "id": "ckjhbvTkFU2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2° - Filter by equations**"
      ],
      "metadata": {
        "id": "xLLw6PipG5e0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2.2 - Filter equation tendency**\n",
        "\n",
        "The tendency equation is a rule that expresses the spectral signature of the soil. The spectral bands Blue, Green, Red, and NIR must necessarily follow the order of increasing values. This order is: Blue < Green < Red < NIR."
      ],
      "metadata": {
        "id": "HVGp23KkpERT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Setting the working directory\n",
        "os.chdir(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania\")\n",
        "\n",
        "# Reading the CSV file\n",
        "DB_raw = pd.read_csv(\"DB_Worldspecs_multi_raw_Oceania_8.csv\", sep=',', encoding='ISO-8859-1')\n",
        "\n",
        "# Exibindo o DataFrame incial\n",
        "print(DB_raw)"
      ],
      "metadata": {
        "id": "CXacz4QdUM0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating the new DataFrame\n",
        "results_equation_tendency = pd.DataFrame()\n",
        "\n",
        "# Adding the 'ID_Unique' column to the new DataFrame\n",
        "results_equation_tendency['ID_Unique'] = DB_raw['ID_Unique']\n",
        "\n",
        "############################################################################\n",
        "# Change the names of the spectral bands, Blue, Green, Red, and NIR, if\n",
        "# the names of these respective bands in the columns of your dataset\n",
        "# are different.\n",
        "\n",
        "# Adding the 'outlier_tendency' column based on the condition\n",
        "condition = (\n",
        "    (DB_raw['Blue_SySI_Sentinel'] < DB_raw['Green_SySI_Sentinel']) &\n",
        "    (DB_raw['Green_SySI_Sentinel'] < DB_raw['Red_SySI_Sentinel']) &\n",
        "    (DB_raw['Red_SySI_Sentinel'] < DB_raw['NIR_SySI_Sentinel'])\n",
        ")\n",
        "############################################################################\n",
        "\n",
        "results_equation_tendency['outlier_tendency'] = ~condition\n",
        "\n",
        "# Displaying the new DataFrame\n",
        "#print(results_equation_tendency)\n",
        "\n",
        "# Adding the 'outlier_tendency' column to the original DataFrame using merge\n",
        "DB_tendency = pd.merge(DB_raw, results_equation_tendency[['ID_Unique', 'outlier_tendency']], on='ID_Unique')\n",
        "\n",
        "# Storing the 'outlier_tendency' column in a temporary variable\n",
        "outlier_tendency_col = DB_tendency['outlier_tendency']\n",
        "\n",
        "# Removing the 'outlier_tendency' column from the DataFrame\n",
        "DB_tendency = DB_tendency.drop('outlier_tendency', axis=1)\n",
        "\n",
        "# Inserting the 'outlier_tendency' column at the desired position (position 2)\n",
        "DB_tendency.insert(2, 'outlier_tendency', outlier_tendency_col)\n",
        "\n",
        "# DataFrame for True values in 'outlier_tendency'\n",
        "outliers_samples_tendency = DB_tendency[DB_tendency['outlier_tendency'] == True]\n",
        "\n",
        "# DataFrame for False values in 'outlier_tendency'\n",
        "DB_filter_tendency = DB_tendency[DB_tendency['outlier_tendency'] == False]\n",
        "\n",
        "# Displaying the final DataFrame\n",
        "print(DB_filter_tendency)\n"
      ],
      "metadata": {
        "id": "bMyBdXylo-hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of samples in the initial dataframe,\n",
        "# outliers tendency dataframe, and filtered dataframe\n",
        "\n",
        "# Print the number of samples in DB_filter_lignin\n",
        "print(\"Number of samples in DB_raw:\", DB_raw.shape[0])\n",
        "\n",
        "# Print the number of samples in outliers_samples_tendency\n",
        "print(\"Number of samples in outliers_samples_tendency:\", outliers_samples_tendency.shape[0])\n",
        "\n",
        "# Print the number of samples in DB_filter_tendency\n",
        "print(\"Number of samples in DB_filter_tendency:\", DB_filter_tendency.shape[0])"
      ],
      "metadata": {
        "id": "6s_hb_3qpKEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_spectra(data, plot_name: str = 'spectra_plot'):\n",
        "    # Dicionário para mapear nomes de colunas para rótulos mais curtos\n",
        "    label_map = {\n",
        "        'Blue_SySI_Sentinel': 'Blue (458-523 nm)',\n",
        "        'Green_SySI_Sentinel': 'Green (543-578 nm)',\n",
        "        'Red_SySI_Sentinel': 'Red (650-680 nm)',\n",
        "        'Red_Edge_1_10m_SySI_Sentinel': 'Red Edge 1 (698-713 nm)',\n",
        "        'Red_Edge_2_10m_SySI_Sentinel': 'Red Edge 2 (733-748 nm)',\n",
        "        'Red_Edge_3_10m_SySI_Sentinel': 'Red Edge 3 (773-793 nm)',\n",
        "        'NIR_SySI_Sentinel': 'NIR (785-899 nm)',\n",
        "        'Red_Edge_4_10m_SySI_Sentinel': 'Red Edge 4 (855-875 nm)',\n",
        "        'SWIR_1_10m_SySI_Sentinel': 'SWIR 1 (1565-1655 nm)',\n",
        "        'SWIR_2_10m_SySI_Sentinel': 'SWIR 2 (2100-2280 nm)'\n",
        "    }\n",
        "\n",
        "    # Especificar as colunas espectrais\n",
        "    spectral_columns = list(label_map.keys())\n",
        "\n",
        "    # Filtrar colunas que existem no DataFrame\n",
        "    existing_columns = [col for col in spectral_columns if col in data.columns]\n",
        "    # Obter rótulos curtos\n",
        "    short_labels = [label_map[col] for col in existing_columns]\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "\n",
        "    # Iterando sobre cada linha nos dados\n",
        "    for index, row in data.iterrows():\n",
        "        # Plotar cada linha com base nas colunas espectrais\n",
        "        plt.plot(short_labels, row[existing_columns], label=f'Outlier {index}')\n",
        "\n",
        "    plt.xlabel('Spectral Band')\n",
        "    plt.ylabel('Reflectance Factor')\n",
        "    plt.title(f'Spectral Signatures of {plot_name}')\n",
        "\n",
        "    # Rotacionar os labels do eixo x para evitar sobreposição\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Definir limites para o eixo y\n",
        "    plt.ylim(0, 0.8)  # Ajuste conforme necessário\n",
        "\n",
        "    plt.grid(False)\n",
        "\n",
        "    # Salvar o gráfico com alta resolução\n",
        "    output_path = \"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/DB_Raw.png\"\n",
        "    dpi_value = 1200\n",
        "    plt.savefig(output_path, dpi=dpi_value, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "rJvnGll-ELAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_spectra(DB_filter_tendency, 'DB Raw')"
      ],
      "metadata": {
        "id": "J-jTGhR8Errq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DB_filter_step_2 DataFrame as a CSV file\n",
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering\")\n",
        "\n",
        "DB_filter_tendency.to_csv(\"DB_filter_step_2_SySI_data.csv\", index=False)\n",
        "outliers_samples_tendency.to_csv(\"outliers_samples_tendency.csv\", index=False)"
      ],
      "metadata": {
        "id": "euxQXewvpNQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3° - Filter by PCA and distance Mahalanobis**"
      ],
      "metadata": {
        "id": "PaeMkL81u3c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.stats import chi2\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import os"
      ],
      "metadata": {
        "id": "Y5seQqAzvA23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the working directory\n",
        "os.chdir(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania\")\n",
        "\n",
        "# Importing soil spectra\n",
        "DB_filter_step_2 = pd.read_csv(\"DB_wosis_202312_clay_orgc_5_Oceania.csv\", sep=',', encoding='ISO-8859-1')\n",
        "DB_filter_step_2 = DB_filter_step_2.set_index('ID_Unique') #Assigning row names from ID column\n",
        "\n",
        "print(DB_filter_step_2)"
      ],
      "metadata": {
        "id": "i89-iUVx66fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### spectra selection ###\n",
        "\n",
        "# Selecting specific columns where the spectrum begins and ends\n",
        "\n",
        "# Specify the range of spectral columns\n",
        "wavelength_min = \"Blue_SySI_Sentinel\" # adjust the min spectral wavelength\n",
        "wavelength_max = \"SWIR_2_10m_SySI_Sentinel\" # adjust the max spectral wavelength\n",
        "\n",
        "# Select the spectral columns\n",
        "data = DB_filter_step_2.loc[:, wavelength_min:wavelength_max] # soil spectra (original)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "XXbTWEGu-c1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### standardized data ###\n",
        "\n",
        "data_selected = data.apply(pd.to_numeric, errors='coerce')\n",
        "data_selected.fillna(data_selected.min(), inplace=True)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = MinMaxScaler()\n",
        "data_standardized = pd.DataFrame(scaler.fit_transform(data), index=data.index)\n",
        "\n",
        "# Convert the standardized data back to a DataFrame\n",
        "data_standardized_df = pd.DataFrame(data_standardized)"
      ],
      "metadata": {
        "id": "2KD1PVn17C1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_standardized)"
      ],
      "metadata": {
        "id": "d12pNLhTaNAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IDENTIY OUTLIERS FUNCTION\n",
        "def identify_outliers(x, n_pc_comp=0):\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    x_scaled = scaler.fit_transform(x)\n",
        "\n",
        "    # Handle NaN values (replace with mean of each column)\n",
        "    x_scaled = np.nan_to_num(x_scaled, nan=np.nanmean(x_scaled, axis=0))\n",
        "\n",
        "    # Perform PCA\n",
        "    pca = PCA()\n",
        "    pr_scores = pca.fit_transform(x_scaled)\n",
        "    explained_variance = pca.explained_variance_ratio_\n",
        "    cpr = np.cumsum(explained_variance)\n",
        "\n",
        "    # Determine number of principal components\n",
        "    if n_pc_comp == 0:\n",
        "        n_pc_comp = np.min(np.where(cpr > 0.999)[0] + 1)\n",
        "    elif 0 < n_pc_comp < 1:\n",
        "        n_pc_comp = np.min(np.where(cpr > n_pc_comp)[0] + 1)\n",
        "\n",
        "    # Calculate mean and covariance\n",
        "    mean_pcaA = np.mean(pr_scores[:, :n_pc_comp], axis=0)\n",
        "    cov_pcaA = np.cov(pr_scores[:, :n_pc_comp], rowvar=False)\n",
        "\n",
        "    # Calculate Mahalanobis distance\n",
        "    chiMat = np.zeros((pr_scores.shape[0], 3))\n",
        "    chiMat[:, 0] = np.array([np.dot(np.dot((pr_scores[i, :n_pc_comp] - mean_pcaA), np.linalg.inv(cov_pcaA)), (pr_scores[i, :n_pc_comp] - mean_pcaA).T) for i in range(pr_scores.shape[0])])\n",
        "\n",
        "    # Fit Chi-Squared distribution\n",
        "    chiMat[:, 1] = chi2.cdf(chiMat[:, 0], df=n_pc_comp)\n",
        "\n",
        "    # Determine critical value\n",
        "    if n_pc_comp <= 10:\n",
        "        pcrit = 1 - ((0.24 - 0.003 * n_pc_comp) / np.sqrt(pr_scores.shape[0]))\n",
        "    else:\n",
        "        pcrit = 1 - ((0.25 - 0.0018 * n_pc_comp) / np.sqrt(pr_scores.shape[0]))\n",
        "    pcrit = 0.999\n",
        "    #pcrit = 0.995\n",
        "\n",
        "    # Identify outliers\n",
        "    chiMat[:, 2] = np.where(chiMat[:, 1] >= pcrit, 0, 1)\n",
        "\n",
        "    # Return the unique IDs of the outliers\n",
        "    outliers_indices = np.where(chiMat[:, 2] == 0)[0]\n",
        "    outliers_ids = x.index[outliers_indices]\n",
        "\n",
        "    return outliers_ids\n"
      ],
      "metadata": {
        "id": "UnU884tV7nwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function and passing the original DataFrame\n",
        "outliers_ids = identify_outliers(data, n_pc_comp=0.995)\n",
        "\n",
        "# Exibir os IDs únicos dos outliers\n",
        "print(outliers_ids)"
      ],
      "metadata": {
        "id": "KCgjqr1F7tOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data_step_3 = data.drop(index=outliers_ids)\n",
        "outliers_data_step_3 = data.loc[outliers_ids]\n",
        "\n",
        "print(outliers_data_step_3)"
      ],
      "metadata": {
        "id": "QlxIPBKN8uXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import os\n",
        "\n",
        "def plot_spectra(data, plot_name: str = 'spectra_plot', save_path=None, dpi: int = 1000):\n",
        "    # Dicionário para mapear nomes de colunas para rótulos mais curtos\n",
        "    label_map = {\n",
        "        'Blue_SySI_Sentinel': 'Blue (458-523 nm)',\n",
        "        'Green_SySI_Sentinel': 'Green (543-578 nm)',\n",
        "        'Red_SySI_Sentinel': 'Red (650-680 nm)',\n",
        "        'Red_Edge_1_10m_SySI_Sentinel': 'Red Edge 1 (698-713 nm)',\n",
        "        'Red_Edge_2_10m_SySI_Sentinel': 'Red Edge 2 (733-748 nm)',\n",
        "        'Red_Edge_3_10m_SySI_Sentinel': 'Red Edge 3 (773-793 nm)',\n",
        "        'NIR_SySI_Sentinel': 'NIR (785-899 nm)',\n",
        "        'Red_Edge_4_10m_SySI_Sentinel': 'Red Edge 4 (855-875 nm)',\n",
        "        'SWIR_1_10m_SySI_Sentinel': 'SWIR 1 (1565-1655 nm)',\n",
        "        'SWIR_2_10m_SySI_Sentinel': 'SWIR 2 (2100-2280 nm)'\n",
        "    }\n",
        "    # Especificar as colunas espectrais\n",
        "    spectral_columns = list(label_map.keys())\n",
        "    # Filtrar colunas que existem no DataFrame\n",
        "    existing_columns = [col for col in spectral_columns if col in data.columns]\n",
        "    # Obter rótulos curtos\n",
        "    short_labels = [label_map[col] for col in existing_columns]\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Iterando sobre cada linha nos dados\n",
        "    for index, row in data.iterrows():\n",
        "        # Plotar cada linha com base nas colunas espectrais\n",
        "        plt.plot(short_labels, row[existing_columns], label=f'Outlier {index}')\n",
        "\n",
        "    plt.xlabel('Spectral Band')\n",
        "    plt.ylabel('Reflectance Factor')\n",
        "    plt.title(f'Spectral Signatures of {plot_name}')\n",
        "\n",
        "    # Rotacionar os labels do eixo x para evitar sobreposição\n",
        "    plt.xticks(rotation=45)\n",
        "    # Definir limites para o eixo y\n",
        "    plt.ylim(0, 1)  # Ajuste conforme necessário\n",
        "    plt.grid(False)\n",
        "    plt.savefig(f'{plot_name}.png', dpi=dpi, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_3d_pca_with_outliers(data_standardized, outliers_ids, save_path=None, dpi: int = 1000):\n",
        "    \"\"\"\n",
        "    Plots a 3D PCA plot highlighting outliers.\n",
        "\n",
        "    Parameters:\n",
        "    - data_standardized: The standardized data (samples x features).\n",
        "    - outliers_ids: Indices of the outliers in the dataset.\n",
        "    - save_path: (Optional) Path to save the plot. If None, the plot is not saved.\n",
        "    \"\"\"\n",
        "    # Handle NaN values before applying PCA\n",
        "    data_standardized = data_standardized.fillna(data_standardized.mean()) # Fill NaN with column means\n",
        "\n",
        "    # Perform PCA\n",
        "    pca = PCA(n_components=3)\n",
        "    pr_scores = pd.DataFrame(pca.fit_transform(data_standardized), index=data_standardized.index)\n",
        "\n",
        "    # Extracting the outliers and non-outliers\n",
        "    outliers = pr_scores.loc[outliers_ids]\n",
        "    non_outliers = pr_scores.drop(index=outliers_ids)\n",
        "\n",
        "    # Convert outliers and non_outliers to numpy arrays for plotting\n",
        "    outliers_array = outliers.to_numpy()\n",
        "    non_outliers_array = non_outliers.to_numpy()\n",
        "\n",
        "    # Create 3D plot\n",
        "    fig = plt.figure(figsize=(15, 13))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Plot outliers\n",
        "    ax.scatter(outliers_array[:, 0], outliers_array[:, 1], outliers_array[:, 2], c='red', marker='x', s=75, label=\"Outliers\")\n",
        "\n",
        "    # Plot non-outliers\n",
        "    ax.scatter(non_outliers_array[:, 0], non_outliers_array[:, 1], non_outliers_array[:, 2], c='gray', marker='o', alpha=0.6,\n",
        "                                                                              edgecolors='black', linewidths=0.45, s=30, label=\"Non-outliers\")\n",
        "\n",
        "    # Label axes with explained variance ratios\n",
        "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0] * 100:.2f}%)', fontweight='bold', fontsize=28, labelpad=20)\n",
        "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1] * 100:.2f}%)', fontweight='bold', fontsize=28, labelpad=28)\n",
        "    ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2] * 100:.2f}%)', fontweight='bold', fontsize=28, labelpad=22)\n",
        "\n",
        "    # Increase the font size of the tick labels\n",
        "    ax.tick_params(axis='both', which='major', labelsize=26, pad=2.8)  # Adjusts x and y axis\n",
        "    ax.tick_params(axis='z', which='major', labelsize=26, pad=7.5)     # Adjusts z axis\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend()\n",
        "\n",
        "    # Adjust layout to prevent label cutoff\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Set the plot title\n",
        "    plt.title('3D PCA plot with Outliers Oceania', fontweight='bold', fontsize=20, y=1.05)\n",
        "\n",
        "    # Save the figure if a path is provided\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=1200)\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "IL-PFaWf80Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots Visualization\n",
        "\n",
        "# Setting the working directory\n",
        "os.chdir(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_3\")\n",
        "\n",
        "plot_spectra(filtered_data_step_3, 'filtered data step 3 Oceania')\n",
        "plot_spectra(outliers_data_step_3, 'outliers Oceania')\n",
        "plot_spectra(data, 'all data Oceania')\n",
        "plot_3d_pca_with_outliers(data, outliers_ids, save_path='3d_pca_outliers_Oceania_multi')"
      ],
      "metadata": {
        "id": "OZYT5tCW84XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers_data_step_3 = outliers_data_step_3.reset_index()\n",
        "\n",
        "IDs_to_filter = outliers_data_step_3['ID_Unique'].tolist()\n",
        "\n",
        "DB_filter_step_3 = DB_filter_step_2.reset_index()\n",
        "\n",
        "# Add a new column \"outliers_step_3\" filled with False\n",
        "DB_filter_step_3.insert(1, \"outliers_step_3\", False)\n",
        "\n",
        "# Update the values to True where ID_Unique is in the list IDs_to_filter\n",
        "DB_filter_step_3.loc[DB_filter_step_3['ID_Unique'].isin(IDs_to_filter), 'outliers_step_3'] = True\n",
        "\n",
        "# Create a new DataFrame with only the rows where outliers_step_3 is True\n",
        "outliers_samples_step_3 = DB_filter_step_3[DB_filter_step_3['outliers_step_3'] == True].copy()\n",
        "\n",
        "# Remove the samples where outliers_step_3 is True\n",
        "DB_filter_step_3 = DB_filter_step_3[DB_filter_step_3['outliers_step_3'] == False].copy()\n",
        "\n",
        "print(outliers_samples_step_3)"
      ],
      "metadata": {
        "id": "TYgANWBE87jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of samples in the initial dataframe,\n",
        "# outliers_samples_step_3 dataframe, and filtered dataframe\n",
        "\n",
        "# Print the number of samples in DB_filter_step_2\n",
        "print(\"Number of samples in DB_filter_step_2:\", DB_filter_step_2.shape[0])\n",
        "\n",
        "# Print the number of samples in outliers_step_3\n",
        "print(\"Number of samples in outliers_samples_step_3:\", outliers_samples_step_3.shape[0])\n",
        "\n",
        "# Print the number of samples in DB_filter_step_3\n",
        "print(\"Number of samples in DB_filter_step_3:\", DB_filter_step_3.shape[0])"
      ],
      "metadata": {
        "id": "RMqQcIGG9NRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the working directory\n",
        "os.chdir(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_3\")\n",
        "\n",
        "# Save the outliers_samples_3 DataFrame as a CSV file\n",
        "outliers_samples_step_3.to_csv(\"outliers_samples_step_3.csv\", index=False)\n",
        "\n",
        "# Save the DB_filter_3 DataFrame as a CSV file\n",
        "DB_filter_step_3.to_csv(\"DB_multi_filter_step_3_Oceania_10.csv\", index=False)"
      ],
      "metadata": {
        "id": "EjQfz5Yq9kbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visual analysis of the outiliers determined by Step 3°**\n",
        "\n",
        "This sub-step is not mandatory for completing the entire protocol, but we strongly recommend that the spectral signatures of samples identified as outliers be visually analyzed. This analysis allows the pedometrist to understand the nature of the outlier spectrum and why the data might be inconsistent or incorrect. Examples include:\n",
        "\n",
        "- Miscalibration of the Vis-NIR-SWIR sensor, resulting in noise in the spectral signature.\n",
        "- Contamination of the soil sample with other materials.\n",
        "- Improper preparation of the soil sample.\n",
        "- Collection of the spectral data from a target other than the soil sample.\n",
        "\n",
        "By understanding the nature of the error, the pedometrist can intervene at the source, preventing similar errors from occurring in the future.\n",
        "However, it is important to note that the spectral data may be correct. The combination of PCA and Mahalanobis distance may flag an outlier that actually corresponds to a soil with distinct spectral behavior, which is not well-represented in the dataset. Therefore, if you identify a sample whose spectral behavior matches all the correct spectral characteristics of a soil in the \"outliers_samples_step_3\" dataset, we recommend including this sample's data in the \"DB_filter_step_3\" dataset.\n",
        "\n",
        "The following interactive plots of the hyperspectral data from soil samples will be generated for visual analysis."
      ],
      "metadata": {
        "id": "43pW8aYm125B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "lmIznPTy2C1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outliers_samples_step_3)"
      ],
      "metadata": {
        "id": "jJSefgi78cag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the indices of the columns you want to select\n",
        "col_index_min = 109  # Index of the first spectral column\n",
        "col_index_max = 2189  # Index of the last spectral column\n",
        "\n",
        "# Select the ID column by name and the spectral columns by indices\n",
        "selected_columns = [outliers_samples_step_3.columns.get_loc('ID_Unique')] + list(range(col_index_min, col_index_max + 1))\n",
        "\n",
        "# Create a new DataFrame with the desired columns\n",
        "DB_graphic = outliers_samples_step_3.iloc[:, selected_columns].copy()\n",
        "\n",
        "print(DB_graphic)"
      ],
      "metadata": {
        "id": "QS4bt6Km8oqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataframe into smaller dataframes based on rows, in this case 50 rows\n",
        "dfs = np.array_split(DB_graphic, np.ceil(len(DB_graphic) / 100))\n",
        "\n",
        "# Create the line chart using Plotly Express for each smaller dataframe\n",
        "for i, df in enumerate(dfs):\n",
        "    # Reshape the dataframe\n",
        "    df_melt = df.melt(id_vars=df.columns[0], value_vars=df.columns[1:])\n",
        "\n",
        "    fig = px.line(df_melt, x='variable', y='value', color=df.columns[0],\n",
        "                  title=f'Original Spectra {i+1}', labels={'variable': 'Wavelength (nm)', 'value': 'Reflectance Factor'})\n",
        "\n",
        "    # Customize the chart layout\n",
        "    fig.update_layout(\n",
        "        xaxis_title='Wavelength (nm)',\n",
        "        yaxis_title='Reflectance Factor',\n",
        "    )\n",
        "\n",
        "    # Build the HTML file name based on the chart\n",
        "    html_file_name = os.path.join('G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/Step_3', f'Original_Spectra_Plotly_{i+1}.html')\n",
        "\n",
        "    # Save the interactive chart to an HTML file\n",
        "    fig.write_html(html_file_name)\n",
        "\n",
        "    # Display the interactive chart\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "KHUNBjnn-_SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4° - Filter by Soil Line**"
      ],
      "metadata": {
        "id": "sBZiuz6M93QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from plotnine import *\n",
        "from plotnine import ggsave\n",
        "import scipy.stats as stats\n",
        "from plotnine import ggplot"
      ],
      "metadata": {
        "id": "4pHaPhQA-Kkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Soil line on convolved spectra ###\n",
        "\n",
        "# Set the working directory\n",
        "os.chdir(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_3\")\n",
        "\n",
        "# Read the CSV file\n",
        "DB_filter_step_3 = pd.read_csv(\"DB_multi_filter_step_3_Oceania_10.csv\")\n",
        "\n",
        "# Assigning to a variable for clarity\n",
        "DB = DB_filter_step_3\n",
        "\n",
        "print(DB_filter_step_3)"
      ],
      "metadata": {
        "id": "vN32n1N9_Pex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cutting line determination ###\n",
        "\n",
        "# Specify the X and Y columns\n",
        "column_x_conv = DB['Red_SySI_Sentinel']\n",
        "column_y_conv = DB['NIR_SySI_Sentinel']\n",
        "\n",
        "# Create a linear regression model\n",
        "X = sm.add_constant(column_x_conv)  # Add the constant for the intercept term\n",
        "model = sm.OLS(column_y_conv, X).fit()\n",
        "\n",
        "# Calculate predicted values\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Calculate the standard deviation of residuals\n",
        "residuals = model.resid\n",
        "std_residuals = np.std(residuals)\n",
        "\n",
        "# Confidence level (e.g., 98%)\n",
        "confidence_level = 0.99\n",
        "\n",
        "# Quantile of the t distribution\n",
        "quantile_t = stats.t.ppf((1 + confidence_level) / 2, df=len(residuals) - 2)\n",
        "\n",
        "# Calculate upper and lower confidence limits\n",
        "upper_limit_NIR_resam = predictions + quantile_t * (std_residuals * 1.35)\n",
        "lower_limit_NIR_resam = predictions - quantile_t * (std_residuals * 1.35)\n",
        "\n",
        "# Regression coefficients\n",
        "intercept, coef_x = model.params\n",
        "\n",
        "# Standard error of coefficients\n",
        "std_error = model.bse\n",
        "\n",
        "# T-value and p-value of coefficients\n",
        "t_value = model.tvalues\n",
        "p_value = model.pvalues\n",
        "\n",
        "# Coefficient of determination (R²)\n",
        "r_squared_convolved = model.rsquared\n",
        "\n",
        "# Fit a linear regression model for the upper limit\n",
        "model_upper_limit = sm.OLS(upper_limit_NIR_resam, X).fit()\n",
        "\n",
        "# Fit a linear regression model for the lower limit\n",
        "model_lower_limit = sm.OLS(lower_limit_NIR_resam, X).fit()\n",
        "\n",
        "# Coefficients of intercept and slope for the upper limit\n",
        "coef_intercept_upper, coef_slope_upper = model_upper_limit.params\n",
        "\n",
        "# Coefficients of intercept and slope for the lower limit\n",
        "coef_intercept_lower, coef_slope_lower = model_lower_limit.params\n",
        "\n",
        "# Display coefficients\n",
        "print(\"Coefficients for Upper Limit:\")\n",
        "print(\"Intercept (β0):\", round(coef_intercept_upper, 4))\n",
        "print(\"Slope (β1):\", round(coef_slope_upper, 4))\n",
        "\n",
        "print(\"\\nCoefficients for Lower Limit:\")\n",
        "print(\"Intercept (β0):\", round(coef_intercept_lower, 4))\n",
        "print(\"Slope (β1):\", round(coef_slope_lower, 4))"
      ],
      "metadata": {
        "id": "mmleqpx4_aGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### plot the Soil line by convolved data ###\n",
        "\n",
        "# Desired width and height of the plot\n",
        "plot_width = 9\n",
        "plot_height = 6\n",
        "\n",
        "soil_line_plot = (\n",
        "    ggplot(DB, aes(x='Red_SySI_Sentinel', y='NIR_SySI_Sentinel')) +\n",
        "    theme_classic() +\n",
        "    geom_point(shape='o', color= 'black', fill= \"#E49247\", alpha=1, size=1.5, stroke=0.2) + # Data points with black contour +  # Data points\n",
        "    geom_smooth(method='lm', se=False, color='#000000', size=1.2) +  # Trend line\n",
        "    geom_line(aes(y=upper_limit_NIR_resam), color='#E41A1C', size=1) +  # Upper confidence interval line\n",
        "    geom_line(aes(y=lower_limit_NIR_resam), color='#E41A1C', size=1) +  # Lower confidence interval line\n",
        "\n",
        "    annotate('text', x=0.11, y=0.58, label=\"β upper limit: {:.3f}\".format(round(coef_intercept_upper, 3)),\n",
        "             family=\"Arial\", size=10)+\n",
        "    annotate('text', x=0.11, y=0.54, label=\"β lower limit: {:.3f}\".format(round(coef_intercept_lower, 3)),\n",
        "             family=\"Arial\", size=10)+\n",
        "    annotate('text', x=0.11, y=0.50, label=\"R² model: {:.3f}\".format(round(r_squared_convolved, 3)),\n",
        "             family=\"Arial\", size=10)+\n",
        "    annotate('text', x=0.11, y=0.46, label=\"α: {:.3f}\".format(round(coef_x, 3)),\n",
        "             family=\"Arial\", size=10)+\n",
        "\n",
        "    labs(x=\"SySI Red band (650-680 nm)\", y=\"SySI NIR band (785-899 nm)\") +\n",
        "    expand_limits(x=0, y=0) +  # Force the plot to show the origin (0,0)\n",
        "    scale_y_continuous(limits=[0, 0.6]) +\n",
        "    scale_x_continuous(limits=[0, 0.6]) +\n",
        "    theme(\n",
        "        axis_title_x=element_text(margin={'t': 10}, family=\"Arial\", size=12, face=\"bold\"),\n",
        "        axis_title_y=element_text(margin={'r': 10}, family=\"Arial\", size=12, face=\"bold\"),\n",
        "        axis_text_y=element_text(family=\"Arial\", size=10),\n",
        "        axis_text_x=element_text(family=\"Arial\", size=10)\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "# Display the plot\n",
        "print(soil_line_plot)\n",
        "\n",
        "dpi_value = 1000\n",
        "\n",
        "# Setting the working directory\n",
        "output_path = \"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_4/soil_line_plot_Oceania.png\"\n",
        "soil_line_plot.save(output_path, width=plot_width, height=plot_height, dpi=dpi_value)\n",
        "\n",
        "soil_line_plot"
      ],
      "metadata": {
        "id": "9SRGiz8t_lz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## After Soil Line analysis, filtering the samples in the database ###\n",
        "\n",
        "# Create a subset of samples that are ABOVE the upper_limit_NIR line\n",
        "samples_upper_limit = DB[DB['NIR_SySI_Sentinel'] > upper_limit_NIR_resam]\n",
        "\n",
        "# Create a subset of samples that are BELOW the lower_limit_NIR line\n",
        "samples_lower_limit = DB[DB['NIR_SySI_Sentinel'] < lower_limit_NIR_resam]\n",
        "\n",
        "# Concatenate the dataframes samples_upper_limit and samples_lower_limit\n",
        "outliers_samples = pd.concat([samples_upper_limit, samples_lower_limit])\n",
        "\n",
        "# Merge the DataFrames based on the ID_Unique column and keep only the columns from the DB DataFrame\n",
        "outliers_samples_step_4 = pd.merge(outliers_samples[['ID_Unique']], DB, on='ID_Unique', how='inner')\n",
        "\n",
        "IDs_to_filter = outliers_samples['ID_Unique'].tolist()\n",
        "\n",
        "DB_filter_step_4 = DB_filter_step_3\n",
        "\n",
        "# Add a new column \"outliers_step_4\" filled with False\n",
        "DB_filter_step_4.insert(1, \"outliers_step_4\", False)\n",
        "\n",
        "# Update the values to True where ID_Unique is in the list IDs_to_filter\n",
        "DB_filter_step_4.loc[DB_filter_step_4['ID_Unique'].isin(IDs_to_filter), 'outliers_step_4'] = True\n",
        "\n",
        "# Create a new DataFrame with only the rows where outliers_step_4 is True\n",
        "outliers_samples_step_4 = DB_filter_step_4[DB_filter_step_4['outliers_step_4'] == True].copy()\n",
        "\n",
        "# Delete the samples where outliers_step_3 is True\n",
        "DB_filter_step_4 = DB_filter_step_4[DB_filter_step_4['outliers_step_4'] == False].copy()\n",
        "\n",
        "print(outliers_samples_step_4)"
      ],
      "metadata": {
        "id": "Yp4IfsPU_7yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of samples in the initial dataframe of the attribute that was analyzed,\n",
        "# outliers outliers_samples_step_4, and DB_filter_step_4 dataframe\n",
        "\n",
        "# Print the number of samples in DB_filter_step_3\n",
        "print(\"Number of samples in DB_filter_step_3:\", DB_filter_step_3.shape[0])\n",
        "\n",
        "# Print the number of samples in outliers_samples_step_4\n",
        "print(\"Number of samples in outliers_samples_step_4:\", outliers_samples_step_4.shape[0])\n",
        "\n",
        "# Print the number of samples in DB_filter_step_4\n",
        "print(\"Number of samples in DB_filter_step_4:\", DB_filter_step_4.shape[0])"
      ],
      "metadata": {
        "id": "j52kT6UpAAd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DB_filter_step_4 and outliers_samples_step_4 DataFrame as a CSV file\n",
        "# Setting the working directory\n",
        "os.chdir(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_4\")\n",
        "\n",
        "outliers_samples_step_4.to_csv(\"outliers_samples_step_4_SySI_data.csv\", index=False)\n",
        "\n",
        "DB_filter_step_4.to_csv(\"DB_multi_filter_step_4_Oceania_11.csv\", index=False)"
      ],
      "metadata": {
        "id": "igpJ2EbzAFuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5° - CIFOD Filter**"
      ],
      "metadata": {
        "id": "_6shgha0ASFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## URSSA module 1: Importing and pre-processing data"
      ],
      "metadata": {
        "id": "eNcXebwYA-qR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import interpolate\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.interpolate import interp1d\n",
        "from plotnine import ggplot"
      ],
      "metadata": {
        "id": "cVSoMaaKBBaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the working directory\n",
        "os.chdir(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_4\")\n",
        "\n",
        "# Importing data (soil attributes + spectra)\n",
        "Data_all = pd.read_csv(\"DB_multi_filter_step_4_Oceania_11.csv\")"
      ],
      "metadata": {
        "id": "8s1W9xFKBL1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Data_all)"
      ],
      "metadata": {
        "id": "k2HUVenuShJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copia o DataFrame Data_all para Data\n",
        "Data = Data_all.copy()\n",
        "\n",
        "wavelength_min = \"Blue_SySI_Sentinel\" # adjust the min spectral wavelength\n",
        "wavelength_max = \"SWIR_2_10m_SySI_Sentinel\" # adjust the max spectral wavelength\n",
        "\n",
        "Spectra_original = Data.loc[:, wavelength_min:wavelength_max] # soil spectra (original)\n",
        "\n",
        "# Seleciona a coluna ID_Unique\n",
        "ID_Unique = Data['ID_Unique']\n",
        "\n",
        "# Cria o DataFrame dataset_model contendo as colunas espectrais e a coluna ID_Unique\n",
        "dataset_model = Spectra_original.copy()\n",
        "\n",
        "# Insere a coluna ID_Unique como a primeira coluna\n",
        "dataset_model.insert(0, 'ID_Unique', ID_Unique)\n",
        "\n",
        "# Plotting of the first ten original spectra\n",
        "plt.figure()\n",
        "plt.plot(Spectra_original.columns, Spectra_original.iloc[:10].T.values)  # Use column names as is (without int conversion)\n",
        "plt.title('Original Spectra')\n",
        "plt.xlabel('Wavelength (nm)')\n",
        "plt.ylabel('Reflectance Factor')\n",
        "plt.xticks(rotation=80)  # Rotate x labels if needed\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gktjX7aF3QWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_model)"
      ],
      "metadata": {
        "id": "0RCvf3lLBewf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## URSSA module 2: Unsupervised spectral clustering of samples\n"
      ],
      "metadata": {
        "id": "R3_x2linBwZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script can be executed in two ways: hosted on a personal machine or through the Google API."
      ],
      "metadata": {
        "id": "RyiEYXAoCN1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***API Google***"
      ],
      "metadata": {
        "id": "oN9Nql3DCcXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr\n",
        "\n",
        "utils = importr('utils')\n",
        "utils.install_packages('randomUniformForest')\n",
        "utils.install_packages('magrittr')\n",
        "utils.install_packages('dplyr')\n",
        "utils.install_packages('tidyverse')"
      ],
      "metadata": {
        "id": "UEEWPu50B0no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### MODULE 2: Unsupervised spectral clustering of samples ####\n",
        "\n",
        "## 1) Unsupervised randomUniformForest classification,\n",
        "## 2) Calculating spectral proximity from URF,\n",
        "## 3) Multidimensional Scaling (MDS),\n",
        "## 4) Clustering samples (K-means)\n",
        "\n",
        "\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr\n",
        "\n",
        "\n",
        "# Import the R packages\n",
        "randomUniformForest = importr('randomUniformForest')\n",
        "magrittr = importr('magrittr')\n",
        "dplyr = importr('dplyr')\n",
        "tidyverse = importr('tidyverse')\n",
        "\n",
        "\n",
        "dataset_model.to_csv(\"dataset_model.csv\", index=True)\n",
        "\n",
        "# Define the R script\n",
        "r_script = \"\"\"\n",
        "\n",
        "setwd(\"/content\")\n",
        "dataset_model <- read.csv(\"dataset_model.csv\", header = TRUE, sep = \",\", dec = \".\", na.strings = \"NA\", check.names = FALSE)\n",
        "dataset_model <- dataset_model %>% remove_rownames %>% column_to_rownames(var=\"ID_Unique\")\n",
        "\n",
        "wavelength_min <- \"350\"\n",
        "wavelength_max <- \"2500\"\n",
        "\n",
        "results <- unsupervised.randomUniformForest(object = dataset_model %>% select(wavelength_min:wavelength_max), # select spectra\n",
        "                                              baseModel = \"proximity\", # c(\"proximity\", \"proximityThenDistance\", \"importanceThenDistance\")\n",
        "                                              endModel = \"MDSkMeans\", # c(\"MDSkMeans\", \"MDShClust\", \"MDS\", \"SpectralkMeans\")\n",
        "                                              endModelMetric = NULL,\n",
        "                                              samplingMethod = \"with bootstrap\", # c(\"uniform univariate sampling\",\"uniform multivariate sampling\", \"with bootstrap\")\n",
        "                                              MDSmetric = \"metricMDS\", # c(\"metricMDS\", \"nonMetricMDS\")\n",
        "                                              proximityMatrix = NULL,\n",
        "                                              sparseProximities = FALSE,\n",
        "                                              outliersFilter = FALSE,\n",
        "                                              Xtest = NULL,\n",
        "                                              predObject = NULL,\n",
        "                                              metricDimension = 2,\n",
        "                                              coordinates = c(1,2),\n",
        "                                              bootstrapReplicates = 100,\n",
        "                                              clusters = NULL,\n",
        "                                              maxIters = NULL,\n",
        "                                              importanceObject = NULL,\n",
        "                                              maxInteractions = 2,\n",
        "                                              reduceClusters = FALSE,\n",
        "                                              maxClusters = 10,\n",
        "                                              mapAndReduce = FALSE,\n",
        "                                              OOB = FALSE,\n",
        "                                              subset = NULL,\n",
        "                                              seed = 2014,\n",
        "                                              uthreads = \"auto\")\n",
        "\n",
        "results_final_R <- data.frame(\"ID_Unique\" = rownames(dataset_model),\n",
        "                              \"cluster\" = results$unsupervisedModel$cluster,\n",
        "                              dataset_model)\n",
        "\n",
        "write.table(results_final_R, \"results_final_R.csv\", sep = \",\", dec = \".\",row.names = FALSE)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Execute the R script\n",
        "robjects.r(r_script)\n"
      ],
      "metadata": {
        "id": "Qvp29D4NDF34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Personal machine - local execution environment:***\n",
        "  Attention, you must indicate the directory path where R is installed. The \"randomUniformForest\" library requires version 4.3.2. And within the script in R language it is necessary to indicate the path to the directory where the libraries are installed."
      ],
      "metadata": {
        "id": "Dgecr7YTDKmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### MODULE 2: Unsupervised spectral clustering of samples ####\n",
        "\n",
        "## 1) Unsupervised randomUniformForest classification,\n",
        "## 2) Calculating spectral proximity from URF,\n",
        "## 3) Multidimensional Scaling (MDS),\n",
        "## 4) Clustering samples (K-means)\n",
        "\n",
        "############################################################################################\n",
        "#  enter the directory of the R language installed on the machine\n",
        "os.environ['R_HOME'] = 'C:/Program Files/R/R-4.3.2'\n",
        "############################################################################################\n",
        "\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr\n",
        "\n",
        "# Saving Dataframes to CSV\n",
        "\n",
        "os.chdir(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_5\")\n",
        "dataset_model.to_csv(\"dataset_model.csv\", index=True)\n",
        "\n",
        "# Define the R script\n",
        "r_script = \"\"\"\n",
        "\n",
        "############################################################################################\n",
        "## R: Directory path to the required hsdar package libraries##                             #\n",
        "#                                                                                          #\n",
        " .libPaths(\"E:/OneDrive/Documentos/Doutorado/protocol_database/libraires\")                 #\n",
        "#                                                                                          #\n",
        "############################################################################################\n",
        "\n",
        "# Load the doParallel library\n",
        "library(doParallel)\n",
        "\n",
        "# Configure parallel cluster (can use desired number of cores)\n",
        "cl <- makeCluster(16)\n",
        "registerDoParallel(cl)\n",
        "\n",
        "library(dplyr)\n",
        "library(tidyverse)\n",
        "library(randomUniformForest)\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "setwd(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_5\")\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "dataset_model <- read.csv(\"dataset_model.csv\", header = TRUE, sep = \",\", dec = \".\", na.strings = \"NA\", check.names = FALSE)\n",
        "dataset_model <- dataset_model %>% remove_rownames %>% column_to_rownames(var=\"ID_Unique\")\n",
        "\n",
        "wavelength_min <- \"Blue_SySI_Sentinel\"\n",
        "wavelength_max <- \"SWIR_2_10m_SySI_Sentinel\"\n",
        "\n",
        "results <- unsupervised.randomUniformForest(object = dataset_model %>% select(wavelength_min:wavelength_max), # select spectra\n",
        "                                              baseModel = \"proximity\", # c(\"proximity\", \"proximityThenDistance\", \"importanceThenDistance\")\n",
        "                                              endModel = \"MDSkMeans\", # c(\"MDSkMeans\", \"MDShClust\", \"MDS\", \"SpectralkMeans\")\n",
        "                                              endModelMetric = NULL,\n",
        "                                              samplingMethod = \"uniform multivariate sampling\", # c(\"uniform univariate sampling\",\"uniform multivariate sampling\", \"with bootstrap\")\n",
        "                                              MDSmetric = \"metricMDS\", # c(\"metricMDS\", \"nonMetricMDS\")\n",
        "                                              proximityMatrix = NULL,\n",
        "                                              sparseProximities = FALSE,\n",
        "                                              outliersFilter = FALSE,\n",
        "                                              Xtest = NULL,\n",
        "                                              predObject = NULL,\n",
        "                                              metricDimension = 2,\n",
        "                                              coordinates = c(1,2),\n",
        "                                              bootstrapReplicates = 100,\n",
        "                                              clusters = NULL,\n",
        "                                              maxIters = 300,\n",
        "                                              importanceObject = NULL,\n",
        "                                              maxInteractions = 5,\n",
        "                                              reduceClusters = FALSE,\n",
        "                                              maxClusters = 7,\n",
        "                                              mapAndReduce = FALSE,\n",
        "                                              OOB = FALSE,\n",
        "                                              subset = NULL,\n",
        "                                              seed = 2014,\n",
        "                                              uthreads = \"auto\")\n",
        "\n",
        "results_final_R <- data.frame(\"ID_Unique\" = rownames(dataset_model),\n",
        "                              \"cluster\" = results$unsupervisedModel$cluster,\n",
        "                              dataset_model)\n",
        "\n",
        "write.table(results_final_R, \"results_final_R.csv\", sep = \",\", dec = \".\",row.names = FALSE)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Execute the R script\n",
        "robjects.r(r_script)"
      ],
      "metadata": {
        "id": "J5AnkCkbDX2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_5\")\n",
        "results_final = pd.read_csv(\"results_final_R.csv\")\n",
        "\n",
        "print(results_final)"
      ],
      "metadata": {
        "id": "r-QvsxQK4A0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### manipulating results in dataframes ###\n",
        "\n",
        "clusters = results_final\n",
        "\n",
        "# Selecting only the 'ID_Unique' and 'cluster' columns from the clusters DataFrame\n",
        "clusters = clusters[['ID_Unique', 'cluster']]\n",
        "\n",
        "# Performing a merge between 'clusters' and 'Data_all' using 'ID_Unique' as the key\n",
        "# This will join both DataFrames, keeping only the rows with matching 'ID_Unique'\n",
        "Soil_data_clustered = pd.merge(clusters, Data_all, on='ID_Unique', how='inner')\n",
        "\n",
        "# Saving the merged DataFrame 'Soil_data_clustered' to a CSV file\n",
        "# The file is saved with a comma as the separator and a dot for decimals\n",
        "Soil_data_clustered.to_csv('Soil_data_clustered.csv', sep=',', decimal='.', index=False)"
      ],
      "metadata": {
        "id": "2jMQa2RfRl3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Identification of soil attribute outliers by Isolation Forest in each cluster"
      ],
      "metadata": {
        "id": "KiIn5_4uED20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from numpy import where\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from plotnine import *"
      ],
      "metadata": {
        "id": "_8PiVdECELoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_5\")\n",
        "Soil_data_clustered = pd.read_csv(\"Soil_data_clustered.csv\")\n",
        "\n",
        "print(Soil_data_clustered)"
      ],
      "metadata": {
        "id": "DteA1rWfEfYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create an empty DataFrame called results\n",
        "results = pd.DataFrame()\n",
        "\n",
        "# Loop through clusters from 1 to 10\n",
        "for i in range(1, 11):\n",
        "    # Filter the DataFrame to select only the records where the \"cluster\" column is equal to i\n",
        "    data_estratificado = Soil_data_clustered[Soil_data_clustered['cluster'] == i]\n",
        "\n",
        "    # Find the columns starting with \"Blue_SySI_Sentinel\" and ending with \"SWIR_2_10m_SySI_Sentinel\"\n",
        "    colunas_selecionadas = data_estratificado.loc[:, 'Blue_SySI_Sentinel':'SWIR_2_10m_SySI_Sentinel']\n",
        "\n",
        "    # Calculate the mean and standard deviation of the selected columns\n",
        "    medias = colunas_selecionadas.mean(numeric_only=True)\n",
        "    desvio_padrao = colunas_selecionadas.std(numeric_only=True)\n",
        "\n",
        "    # Create a temporary DataFrame with the results for the current cluster\n",
        "    results_cluster = pd.DataFrame({\n",
        "        'cluster': [i] * len(medias),\n",
        "        'wavelength': medias.index,  # Keep the column names as strings\n",
        "        'average_reflectance': medias.values,\n",
        "        'sd': desvio_padrao.values\n",
        "    })\n",
        "\n",
        "    # Add the results of the current cluster to the main DataFrame\n",
        "    results = pd.concat([results, results_cluster], ignore_index=True)\n",
        "\n",
        "# Delete rows where 'average_reflectance' is NaN (null)\n",
        "results = results.dropna(subset=['average_reflectance'])\n",
        "\n",
        "# Dicionário para mapear nomes de colunas para rótulos mais curtos\n",
        "label_map = {\n",
        "    'Blue_SySI_Sentinel': 'Blue (458-523 nm)',\n",
        "    'Green_SySI_Sentinel': 'Green (543-578 nm)',\n",
        "    'Red_SySI_Sentinel': 'Red (650-680 nm)',\n",
        "    'Red_Edge_1_10m_SySI_Sentinel': 'Red Edge 1 (698-713 nm)',\n",
        "    'Red_Edge_2_10m_SySI_Sentinel': 'Red Edge 2 (733-748 nm)',\n",
        "    'Red_Edge_3_10m_SySI_Sentinel': 'Red Edge 3 (773-793 nm)',\n",
        "    'NIR_SySI_Sentinel': 'NIR (785-899 nm)',\n",
        "    'Red_Edge_4_10m_SySI_Sentinel': 'Red Edge 4 (855-875 nm)',\n",
        "    'SWIR_1_10m_SySI_Sentinel': 'SWIR 1 (1565-1655 nm)',\n",
        "    'SWIR_2_10m_SySI_Sentinel': 'SWIR 2 (2100-2280 nm)'\n",
        "}\n",
        "\n",
        "# Aplicar o mapeamento de nomes\n",
        "results['wavelength'] = results['wavelength'].replace(label_map)\n",
        "\n",
        "# Definir a ordem correta dos rótulos espectrais\n",
        "ordered_labels = list(label_map.values())\n",
        "\n",
        "# Converter para categórico com ordem específica\n",
        "results['wavelength'] = pd.Categorical(results['wavelength'], categories=ordered_labels, ordered=True)\n",
        "\n",
        "# Create the color palette based on the number of clusters\n",
        "palette = [\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\",\"#FF760A\",\n",
        "           \"#FFD700\",\"#0AB2D1\", \"#F781BF\", \"#999999\", \"#66C2A5\", \"#8C564B\",\n",
        "           \"#1F78B4\", \"#B2DF8A\", \"#FB9A99\", \"#FDBF6F\", \"#CAB2D6\", \"#6A3D9A\"]\n",
        "\n",
        "# Desired width and height of the plot\n",
        "plot_width = 14\n",
        "plot_height = 7\n",
        "\n",
        "spectral_behavior = (ggplot(results, aes(x='wavelength',\n",
        "                                         y='average_reflectance',\n",
        "                                         ymin='average_reflectance - sd',\n",
        "                                         ymax='average_reflectance + sd',\n",
        "                                         group='cluster')) +\n",
        "\n",
        "  geom_line(aes(color='factor(cluster)'), size=0.85) +\n",
        "  geom_ribbon(aes(fill='factor(cluster)'), alpha=0.2) +\n",
        "  scale_y_continuous(limits=[0, 0.6]) +\n",
        "  scale_color_manual(values=palette) +  # Manually set the colors\n",
        "  scale_fill_manual(values=palette) +  # Manually set the fill colors\n",
        "  labs(title='Spectral behavior of clusters Oceania',\n",
        "       x='Wavelength (nm)',\n",
        "       y='Reflectance factor',\n",
        "       color='Cluster',   # Name the color legend\n",
        "       fill='Cluster') +  # Name the fill legend\n",
        "  theme_minimal() +\n",
        "  theme(figure_size=(plot_width, plot_height),\n",
        "        plot_title=element_text(size=15, face=\"bold\"),\n",
        "        axis_title_x=element_text(size=12, face=\"bold\"),\n",
        "        axis_title_y=element_text(size=12, face=\"bold\"),\n",
        "        axis_text_x=element_text(size=10, angle=45, hjust=1),  # Rotate x-axis labels\n",
        "        legend_text=element_text(size=10),\n",
        "        legend_title=element_text(size=12, face=\"bold\"),\n",
        "        plot_background=element_rect(fill=\"white\")))\n",
        "\n",
        "print(spectral_behavior)\n",
        "\n",
        "dpi_value = 1200\n",
        "\n",
        "# Setting the working directory\n",
        "output_path = \"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_5/SySI_spectral_behavior_of_clusters_Oceania.png\"\n",
        "spectral_behavior.save(output_path, width=plot_width, height=plot_height, dpi=dpi_value)\n",
        "\n",
        "#Show spectral_behavior\n",
        "spectral_behavior"
      ],
      "metadata": {
        "id": "mODtL_MOT-Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "Data = Soil_data_clustered.dropna(subset=['C_gkg'])\n",
        "Data = Data.set_index('ID_Unique')\n",
        "\n",
        "# Select the cluster column and the variable of interest\n",
        "Observation = Data[[\"cluster\", \"C_gkg\"]]\n",
        "\n",
        "# Filter null data in the \"Clay_gkg\" column\n",
        "Observation = Observation[Observation['C_gkg'].notnull()]\n",
        "\n",
        "wavelength_min = \"Blue_SySI_Sentinel\" # adjust the min spectral wavelength\n",
        "wavelength_max = \"SWIR_2_10m_SySI_Sentinel\" # adjust the max spectral wavelength\n",
        "\n",
        "Spectra_original = Data.loc[:, wavelength_min:wavelength_max] # soil spectra (original)\n",
        "\n",
        "# Merge DataFrames to have all the information in one place\n",
        "Observation_final = pd.merge(Observation, Spectra_original, left_index=True, right_index=True, how='left')\n",
        "\n",
        "print(Observation_final)"
      ],
      "metadata": {
        "id": "g3i_Yx4xFMe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Observation_final)"
      ],
      "metadata": {
        "id": "gCbL5Nujc9-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############ Isolation Forest ############\n",
        "\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html\n",
        "\n",
        "# Load the dataset\n",
        "data = Observation_final\n",
        "\n",
        "# Remove the 'cluster' column to keep only spectral features\n",
        "features = data.drop(columns=['cluster'])\n",
        "\n",
        "# Initialize dictionaries to store results\n",
        "\n",
        "all_anomaly_dict = {}\n",
        "out_anomalies_dict = {}\n",
        "plot_data_dict = {}\n",
        "\n",
        "# Iterate over each unique cluster\n",
        "for idx, cluster_label in enumerate(sorted(data['cluster'].unique())):\n",
        "\n",
        "    # Filter data for the current cluster\n",
        "    cluster_data = data[data['cluster'] == cluster_label]\n",
        "\n",
        "    # Remove rows with missing values and keep only spectral features\n",
        "    # Use features.columns to get the column names and select only those columns\n",
        "    data_filtered = cluster_data[features.columns].dropna()\n",
        "\n",
        "    # Normalize the data using StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    data_filtered_scaled = scaler.fit_transform(data_filtered)\n",
        "\n",
        "    # Apply Isolation Forest for anomaly detection\n",
        "    iforest = IsolationForest(n_estimators=100, contamination=0.02, random_state=42)\n",
        "    predictions = iforest.fit_predict(data_filtered_scaled)\n",
        "\n",
        "    # Get the anomaly score\n",
        "    anomaly_scores = iforest.decision_function(data_filtered_scaled)\n",
        "    data_filtered['anomaly_score'] = anomaly_scores\n",
        "\n",
        "    # Identify indices of outliers (anomalies)\n",
        "    anom_index = where(predictions == -1)\n",
        "    anomalies = data_filtered.iloc[anom_index]\n",
        "    anomalies['cluster'] = cluster_label  # Add the cluster column\n",
        "\n",
        "    # Store anomalies in the dictionary\n",
        "    out_anomalies_dict[cluster_label] = anomalies\n",
        "\n",
        "    # Store anomaly scores for all samples in the dictionary\n",
        "    data_filtered['cluster'] = cluster_label\n",
        "    all_anomaly_dict[cluster_label] = data_filtered  # Add scores\n",
        "\n",
        "    # Apply PCA to reduce spectral features to 2 principal components\n",
        "    spectral_features = features.columns[1:]  # Exclude 'Clay_gkg'\n",
        "    pca = PCA(n_components=2)\n",
        "    data_pca = pca.fit_transform(data_filtered[spectral_features])\n",
        "\n",
        "    # Create a DataFrame with PCs and the 'Clay_gkg' attribute\n",
        "    data_3d = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])\n",
        "    data_3d['C_gkg'] = data_filtered['C_gkg'].values\n",
        "\n",
        "    # Separate data into outliers and non-outliers\n",
        "    outliers_3d = data_3d.iloc[anom_index]\n",
        "    non_outliers_3d = data_3d.drop(index=anom_index[0])\n",
        "\n",
        "    # Store data for plots in the dictionary\n",
        "    plot_data_dict[cluster_label] = {\n",
        "        'data_3d': data_3d,\n",
        "        'outliers_3d': outliers_3d,\n",
        "        'non_outliers_3d': non_outliers_3d,\n",
        "        'pca': pca\n",
        "    }"
      ],
      "metadata": {
        "id": "AdOsavYk0UYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 3D plot Visualization ###\n",
        "\n",
        "# Define the color palette for the clusters\n",
        "palette = [\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\",\"#FF760A\",\"#FFD700\",\n",
        "           \"#0AB2D1\", \"#F781BF\", \"#999999\", \"#66C2A5\"]\n",
        "\n",
        "# Output directory for the plots\n",
        "output_dir = \"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_5/Carbon\"\n",
        "\n",
        "# Iterate over each cluster and generate plots\n",
        "for idx, (cluster_label, results) in enumerate(plot_data_dict.items()):\n",
        "    data_3d = results['data_3d']\n",
        "    outliers_3d = results['outliers_3d']\n",
        "    non_outliers_3d = results['non_outliers_3d']\n",
        "    pca = results['pca']\n",
        "\n",
        "    # Create 3D plot\n",
        "    fig = plt.figure(figsize=(18, 16))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Plot outliers\n",
        "    ax.scatter(outliers_3d['PC1'], outliers_3d['PC2'], outliers_3d['C_gkg'],\n",
        "               c='red', marker='x', s=110, label=\"Outliers\")\n",
        "\n",
        "    # Plot non-outliers with cluster-specific color\n",
        "    cluster_color = palette[idx % len(palette)]\n",
        "    ax.scatter(non_outliers_3d['PC1'], non_outliers_3d['PC2'], non_outliers_3d['C_gkg'],\n",
        "               c=cluster_color, marker='o', alpha=1, edgecolors='black', s=60, label=\"Non-outliers\")\n",
        "\n",
        "\n",
        "    # Label axes with explained variance of PCs and Clay attribute, and increase font size\n",
        "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0] * 100:.2f}%)', fontweight='bold', fontsize=35, labelpad=37)\n",
        "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1] * 100:.2f}%)', fontweight='bold', fontsize=35, labelpad=45)\n",
        "    ax.set_zlabel('C gkg', fontweight='bold', fontsize=35, labelpad=54)\n",
        "\n",
        "    # Increase the font size of the tick labels\n",
        "    ax.tick_params(axis='both', which='major', labelsize=33, pad=9)  # Adjusts x and y axis\n",
        "    ax.tick_params(axis='z', which='major', labelsize=33, pad=24)     # Adjusts z axis\n",
        "\n",
        "    # Set z-axis limits from 0 to 900\n",
        "    ax.set_zlim(0, 250)\n",
        "\n",
        "    # Adjust layout to prevent label cutoff\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Add legend and title\n",
        "    ax.legend(fontsize=12)\n",
        "    plt.title(f'3D PCA plot with Outliers for Cluster {cluster_label} Oceania\\n(Organic Carbon gkg vs. PC1 & PC2)', fontweight='bold', fontsize=20, y=1.05)\n",
        "\n",
        "    # Save the plot as an image with specified DPI\n",
        "    output_path = f\"{output_dir}cluster_{cluster_label}_outlier_detection.png\"\n",
        "    plt.savefig(output_path, dpi=1000)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2k9F12861Zjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all DataFrames in the dictionary into a single DataFrame\n",
        "anomalies_combined = pd.concat(out_anomalies_dict.values())\n",
        "\n",
        "# Reset the index of the combined DataFrame\n",
        "anomalies_combined = anomalies_combined.reset_index()\n",
        "\n",
        "print(anomalies_combined)"
      ],
      "metadata": {
        "id": "L5bEfS_l-ZHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a list of all sample IDs in anomalies_combined\n",
        "IDs_to_filter = anomalies_combined['ID_Unique'].tolist()\n",
        "\n",
        "# Print the list of sample IDs\n",
        "print(IDs_to_filter)\n",
        "\n",
        "# Summary of the number of samples per cluster\n",
        "summary = anomalies_combined.groupby('cluster').size()\n",
        "\n",
        "# Print the summary by cluster\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "QHypDiBR17_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all DataFrames in the dictionary into a single DataFrame\n",
        "all_anomaly = pd.concat(all_anomaly_dict.values())\n",
        "\n",
        "# Reset the index of the combined DataFrame\n",
        "all_anomaly = all_anomaly.reset_index()\n",
        "\n",
        "# Add a new column \"outliers\" filled with False\n",
        "all_anomaly.insert(1, \"outliers\", False)\n",
        "\n",
        "# Update the values to True where ID_Unique is in the list IDs_to_filter\n",
        "all_anomaly.loc[all_anomaly['ID_Unique'].isin(IDs_to_filter), 'outliers'] = True\n",
        "\n",
        "print(all_anomaly)\n",
        "\n",
        "# Setting the working directory\n",
        "os.chdir(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_5/Carbon\")\n",
        "\n",
        "# Save all_anomaly DataFrame as a CSV file\n",
        "all_anomaly.to_csv(\"all_anomaly_score.csv\", index=False)"
      ],
      "metadata": {
        "id": "rEsJhUni2ls0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DB_filter_step_5 = Data.reset_index()\n",
        "\n",
        "# Add a new column \"outliers_step_5_\" filled with False\n",
        "DB_filter_step_5.insert(1, \"outliers_step_5_C\", False)\n",
        "\n",
        "# Update the values to True where ID_Unique is in the list IDs_to_filter\n",
        "DB_filter_step_5.loc[DB_filter_step_5['ID_Unique'].isin(IDs_to_filter), 'outliers_step_5_C'] = True\n",
        "\n",
        "# Create a new DataFrame with only the rows where outliers_step_5_ is True\n",
        "outliers_samples_step_5 = DB_filter_step_5[DB_filter_step_5['outliers_step_5_C'] == True].copy()\n",
        "\n",
        "# Delete the samples where outliers_step_5_ is True\n",
        "DB_filter_step_5 = DB_filter_step_5[DB_filter_step_5['outliers_step_5_C'] == False].copy()\n",
        "\n",
        "print(outliers_samples_step_5)"
      ],
      "metadata": {
        "id": "WijBfdPm19tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of samples in the initial dataframe of the attribute that was analyzed,\n",
        "# outliers outliers_samples_step_5, and DB_filter_step_5 dataframe\n",
        "\n",
        "# Print the number of samples in Observation_final\n",
        "print(\"Number of samples in Observation_final:\", Observation_final.shape[0])\n",
        "\n",
        "# Print the number of samples in outliers_samples_step_5_\n",
        "print(\"Number of samples in outliers_samples_step_5_C:\", outliers_samples_step_5.shape[0])\n",
        "\n",
        "# Print the number of samples in DB_filter_step_5_\n",
        "print(\"Number of samples in DB_filter_step_5_C:\", DB_filter_step_5.shape[0])"
      ],
      "metadata": {
        "id": "If4mWN992D5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DB_filter_step_5 and outliers_samples_step_5 DataFrame as a CSV file\n",
        "\n",
        "# Setting the working directory\n",
        "os.chdir(\"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_5/Carbon\")\n",
        "\n",
        "# Save the outliers_samples_3 DataFrame as a CSV file\n",
        "outliers_samples_step_5.to_csv(\"outliers_samples_step_5_C.csv\", index=False)\n",
        "\n",
        "# Save the DB_filter_3 DataFrame as a CSV file\n",
        "DB_filter_step_5.to_csv(\"DB_multi_filter_step_5_C_Oceania_12.csv\", index=False)"
      ],
      "metadata": {
        "id": "T9ngOjBL2Ik7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotnine import *\n",
        "\n",
        "# Create the color palette based on the number of clusters\n",
        "palette = [\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\", \"#FF760A\",\"#FFD700\",\n",
        "           \"#0AB2D1\", \"#F781BF\", \"#999999\", \"#66C2A5\", \"#8C564B\",\"#1F78B4\",\n",
        "           \"#B2DF8A\", \"#FB9A99\", \"#FDBF6F\", \"#CAB2D6\", \"#6A3D9A\"]\n",
        "\n",
        "# Set the desired plot size\n",
        "plot_width = 10\n",
        "plot_height = 9\n",
        "\n",
        "# Create the plot using plotnine\n",
        "box_plot_1 = (ggplot(Observation_final, aes(x='factor(cluster)', y='C_gkg', fill='factor(cluster)')) +\n",
        "            geom_boxplot(alpha=0.7) +\n",
        "            scale_fill_manual(values=palette) +\n",
        "            scale_y_continuous(limits=[0, 250]) +\n",
        "            labs(title='Box Plot of Organic Carbon gkg for Each Cluster before filter',\n",
        "                 x='Cluster',\n",
        "                 y='C gkg') +\n",
        "            theme_minimal() +\n",
        "            theme(figure_size=(plot_width, plot_height),\n",
        "                  plot_title=element_text(size=13, face=\"bold\"),\n",
        "                  axis_title_x=element_text(size=10, face=\"bold\"),\n",
        "                  axis_title_y=element_text(size=10, face=\"bold\"),\n",
        "                  plot_background=element_rect(fill=\"white\")))\n",
        "\n",
        "# Display the plot\n",
        "print(box_plot_1)\n",
        "\n",
        "# Create the plot using plotnine\n",
        "box_plot_2 = (ggplot(DB_filter_step_5, aes(x='factor(cluster)', y='C_gkg', fill='factor(cluster)')) +\n",
        "            geom_boxplot(alpha=0.7) +\n",
        "            scale_fill_manual(values=palette) +\n",
        "            scale_y_continuous(limits=[0, 250]) +\n",
        "            labs(title='Box Plot of Organic Carbon gkg for Each Cluster after filter',\n",
        "                 x='Cluster',\n",
        "                 y='C gkg') +\n",
        "            theme_minimal() +\n",
        "            theme(figure_size=(plot_width, plot_height),\n",
        "                  plot_title=element_text(size=13, face=\"bold\"),\n",
        "                  axis_title_x=element_text(size=10, face=\"bold\"),\n",
        "                  axis_title_y=element_text(size=10, face=\"bold\"),\n",
        "                  plot_background=element_rect(fill=\"white\")))\n",
        "\n",
        "# Display the plot\n",
        "print(box_plot_2)\n",
        "\n",
        "dpi_value = 1000\n",
        "\n",
        "# Setting the working directory\n",
        "output_path = \"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_5/Carbon/box_plot_1_Oceania_multi_Carbon_before_step_5.png\"\n",
        "box_plot_1.save(output_path, width=plot_width, height=plot_height, dpi=dpi_value)\n",
        "\n",
        "output_path = \"E:/OneDrive/Documentos/Doutorado/protocol_database/continents_protocol_validation/multiespectral_data_approach/Oceania/Step_5/Carbon/box_plot_2_Oceania_multi_Carbon_after_step_5.png\"\n",
        "box_plot_2.save(output_path, width=plot_width, height=plot_height, dpi=dpi_value)"
      ],
      "metadata": {
        "id": "0Gt7YTy3IRoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from plotnine import ggplot, aes, geom_boxplot, scale_fill_manual, scale_y_continuous, labs, theme_minimal, theme, element_text, element_rect\n",
        "\n",
        "# Create the color palette based on the number of clusters\n",
        "palette = [\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\", \"#FF7F00\",\n",
        "           \"#FFFF33\", \"#A65628\", \"#F781BF\", \"#999999\", \"#66C2A5\", \"#8C564B\",\n",
        "           \"#1F78B4\", \"#B2DF8A\", \"#FB9A99\", \"#FDBF6F\", \"#CAB2D6\", \"#6A3D9A\"]\n",
        "\n",
        "# Set the desired plot size\n",
        "plot_width = 10\n",
        "plot_height = 9\n",
        "\n",
        "# Create the plot using plotnine for before the filter\n",
        "box_plot_1 = (ggplot(Observation_final, aes(x='factor(cluster)', y='C_gkg', fill='factor(cluster)')) +\n",
        "            geom_boxplot(alpha=0.7) +\n",
        "            scale_fill_manual(values=palette) +\n",
        "            scale_y_continuous(limits=[0, 200]) +\n",
        "            labs(title='Box Plot of C gkg for Each Cluster before filter',\n",
        "                 x='Cluster',\n",
        "                 y='C_gkg') +\n",
        "            theme_minimal() +\n",
        "            theme(figure_size=(plot_width, plot_height),\n",
        "                  plot_title=element_text(size=13, face=\"bold\"),\n",
        "                  axis_title_x=element_text(size=10, face=\"bold\"),\n",
        "                  axis_title_y=element_text(size=10, face=\"bold\"),\n",
        "                  plot_background=element_rect(fill=\"white\")))\n",
        "\n",
        "# Create the plot using plotnine for after the filter\n",
        "box_plot_2 = (ggplot(DB_filter_step_5_C, aes(x='factor(cluster)', y='Clay_gkg', fill='factor(cluster)')) +\n",
        "            geom_boxplot(alpha=0.7) +\n",
        "            scale_fill_manual(values=palette) +\n",
        "            scale_y_continuous(limits=[0, 200]) +\n",
        "            labs(title='Box Plot of C gkg for Each Cluster after filter',\n",
        "                 x='Cluster',\n",
        "                 y='Clay_gkg') +\n",
        "            theme_minimal() +\n",
        "            theme(figure_size=(plot_width, plot_height),\n",
        "                  plot_title=element_text(size=13, face=\"bold\"),\n",
        "                  axis_title_x=element_text(size=10, face=\"bold\"),\n",
        "                  axis_title_y=element_text(size=10, face=\"bold\"),\n",
        "                  plot_background=element_rect(fill=\"white\")))\n",
        "\n",
        "# Save the plots with high resolution (1000 dpi)\n",
        "box_plot_1.save(\"box_plot_1.png\", dpi=1000)\n",
        "box_plot_2.save(\"box_plot_2.png\", dpi=1000)\n",
        "\n",
        "# Load the saved images\n",
        "img1 = mpimg.imread(\"box_plot_1.png\")\n",
        "img2 = mpimg.imread(\"box_plot_2.png\")\n",
        "\n",
        "# Set up the side-by-side display\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(plot_width*2, plot_height))\n",
        "\n",
        "# Display the images side by side\n",
        "axes[0].imshow(img1)\n",
        "axes[0].axis('off')  # Remove the axes\n",
        "axes[0].set_title('Box Plot of C gkg for Each Cluster before filter')\n",
        "\n",
        "axes[1].imshow(img2)\n",
        "axes[1].axis('off')  # Remove the axes\n",
        "axes[1].set_title('Box Plot of C gkg for Each Cluster after filter')\n",
        "\n",
        "# Save the combined figure with high resolution (1000 dpi)\n",
        "plt.tight_layout()\n",
        "plt.savefig('combined_box_plots.png', dpi=1000)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lCCbYpfVISBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6° - ABOPA Filter**"
      ],
      "metadata": {
        "id": "XrMIUcKhMtev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing and manipulating data"
      ],
      "metadata": {
        "id": "NPDBd2rnNDFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.inspection import permutation_importance\n",
        "from joblib import Parallel, delayed\n",
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "6V8NZdnC8vGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Working directory settings\n",
        "os.chdir(\"D:/alunos/Bruno_B/final_application/multiespectral_data_approach/Step_5_SySI_data/Organic_Carbon\")\n",
        "\n",
        "# ===============================================================\n",
        "# DATA READING AND PREPARATION\n",
        "DB_filter_step_5 = pd.read_csv(\"DB_filter_step_5_C_3.csv\", header=0, sep=\",\", decimal=\".\", na_values=[\"NA\", \"\"])\n",
        "\n",
        "DB = DB_filter_step_5.copy()"
      ],
      "metadata": {
        "id": "kaL9HDAO9D7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DB)"
      ],
      "metadata": {
        "id": "m12pbXrJ9HbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting specific columns where the spectrum begins and ends\n",
        "\n",
        "# Specify the range of spectral columns\n",
        "wavelength_min = \"Blue_SySI_Sentinel\" # adjust the min spectral wavelength\n",
        "wavelength_max = \"SWIR_2_10m_SySI_Sentinel\" # adjust the max spectral wavelength\n",
        "\n",
        "# Select the spectral columns\n",
        "DB_spectra = DB.loc[:, wavelength_min:wavelength_max] # soil spectra (original)\n",
        "\n",
        "print(DB_spectra)"
      ],
      "metadata": {
        "id": "X4mut3Jc9J7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new dataframe with specific columns and removing rows with NA values\n",
        "\n",
        "DB1 = pd.concat([DB.iloc[:, [0, 9]],  # Select the column number where the dependent variable is\n",
        "\n",
        "                DB_spectra.iloc[:, list(range(0, 10, 1))]], axis=1)\n",
        "\n",
        "DB1 = DB1.dropna()\n",
        "\n",
        "# Printing the dataframe structure\n",
        "print(DB1.info())\n",
        "\n",
        "print(DB1)\n",
        "\n",
        "# Separation of predictors and response variable\n",
        "X = DB1.iloc[:, 2:11].values  # Covariates (columns 3 to 12)\n",
        "y = DB1['C_gkg'].values        # Response variable"
      ],
      "metadata": {
        "id": "XrSuO43N9L16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Random Forest algorithm:***"
      ],
      "metadata": {
        "id": "Xw3BxVtr9SOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# AUXILIARY FUNCTIONS\n",
        "# ===============================================================\n",
        "\n",
        "def train_fold_model(model, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Trains the model on a fold and calculates evaluation metrics.\n",
        "    \"\"\"\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    r2 = r2_score(y_val, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    rpiq = (np.percentile(y_val, 75) - np.percentile(y_val, 25)) / rmse\n",
        "    return {'R2': r2, 'RMSE': rmse, 'RPIQ': rpiq}, model.feature_importances_\n",
        "\n",
        "def summarize_fold_metrics(fold_metrics):\n",
        "    \"\"\"\n",
        "    Calculates the average metrics across folds.\n",
        "    \"\"\"\n",
        "    fold_metrics_df = pd.DataFrame(fold_metrics)\n",
        "    avg_metrics = {\n",
        "        'Average R2': fold_metrics_df['R2'].mean(),\n",
        "        'Average RMSE': fold_metrics_df['RMSE'].mean(),\n",
        "        'Average RPIQ': fold_metrics_df['RPIQ'].mean()\n",
        "    }\n",
        "    return avg_metrics, fold_metrics_df\n",
        "\n",
        "def calculate_feature_importances(importances, feature_names):\n",
        "    \"\"\"\n",
        "    Calculates the mean feature importances.\n",
        "    \"\"\"\n",
        "    mean_importances = np.mean(importances, axis=0)\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Mean Importance': mean_importances\n",
        "    }).sort_values(by='Mean Importance', ascending=False)\n",
        "    return importance_df"
      ],
      "metadata": {
        "id": "dnzm7Bug9TTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# INITIAL SETTINGS\n",
        "# ===============================================================\n",
        "# Initializing lists to store results\n",
        "final_metrics = []\n",
        "final_importances = []\n",
        "final_predictions = []\n",
        "\n",
        "# Set up the Random Forest model\n",
        "define_model = RandomForestRegressor(random_state=None, n_jobs=5)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [30, 100, 200, 500],\n",
        "    'max_features': [2, 6],  # Number of variables per split\n",
        "    'min_samples_leaf': [3, 6, 9],  # Minimum population in the leaf\n",
        "    'max_leaf_nodes': [500, 1000, 1500],  # Maximum number of leaves\n",
        "    'bootstrap': [True]\n",
        "}\n",
        "\n",
        "# Set up cross-validation (10-fold)\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=None)"
      ],
      "metadata": {
        "id": "D_cBxEXv9W9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# MODEL TRAINING\n",
        "# ===============================================================\n",
        "for model_num in range(50):\n",
        "    print(f\"\\nTraining Model {model_num + 1}...\")\n",
        "\n",
        "    # Configure Grid Search with cross-validation\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=define_model,\n",
        "        param_grid=param_grid,\n",
        "        cv=kf,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        n_jobs=60,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Fit the model and search for the best hyperparameters\n",
        "    grid_search.fit(X, y)\n",
        "    best_params = grid_search.best_params_\n",
        "    print(f\"Best hyperparameters for Model {model_num + 1}: {best_params}\")\n",
        "\n",
        "    # Best model with the optimal hyperparameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Model evaluation per fold\n",
        "    fold_metrics = []\n",
        "    importances = []\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(X), 1):\n",
        "        # Split data into training and validation\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        # Train and evaluate the model on the current fold\n",
        "        metrics, feature_importances = train_fold_model(\n",
        "            best_model, X_train, y_train, X_val, y_val\n",
        "        )\n",
        "\n",
        "        # Store fold results\n",
        "        fold_metrics.append({'Fold': fold, **metrics})\n",
        "        importances.append(feature_importances)\n",
        "\n",
        "    # Summary of metrics by model\n",
        "    avg_metrics, fold_metrics_df = summarize_fold_metrics(fold_metrics)\n",
        "    avg_metrics['Model'] = model_num + 1\n",
        "    print(f\"\\nFold Metrics for Model {model_num + 1}:\")\n",
        "    print(tabulate(fold_metrics_df, headers='keys', tablefmt='pretty'))\n",
        "\n",
        "    print(f\"\\nAverage Metrics for Model {model_num + 1}:\")\n",
        "    print(tabulate([avg_metrics], headers='keys', tablefmt='pretty'))\n",
        "\n",
        "    # Feature importance\n",
        "    feature_names = DB1.columns[2:11]  # Names of the covariates\n",
        "    importance_df = calculate_feature_importances(importances, feature_names)\n",
        "    print(f\"\\nFeature Importance for Model {model_num + 1}:\")\n",
        "    print(tabulate(importance_df, headers='keys', tablefmt='pretty'))\n",
        "\n",
        "    # Store results\n",
        "    final_metrics.append(avg_metrics)\n",
        "    final_importances.append(importance_df)\n",
        "\n",
        "    # Train the final model with all data\n",
        "    best_model.fit(X, y)\n",
        "    final_predictions.append(best_model.predict(X))"
      ],
      "metadata": {
        "id": "AFiMs6T79vtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# SUMMARY OF COVARIATE IMPORTANCE IN EACH ITERATION\n",
        "# ===============================================================\n",
        "\n",
        "# Initializing the list to store feature importance DataFrames\n",
        "dfs_Mean_feature_importance = []\n",
        "\n",
        "# Assuming 'final_importances' contains the DataFrames of feature importances\n",
        "for i, df_fi in enumerate(final_importances):\n",
        "    renamed = df_fi.rename(columns={\"Mean Importance\": f\"Mean_Importance_Model_{i+1}\"})\n",
        "    dfs_Mean_feature_importance.append(renamed)\n",
        "\n",
        "# Consolidating feature importances into a single DataFrame\n",
        "all_Mean_feature_importance = dfs_Mean_feature_importance[0]\n",
        "for df_fi in dfs_Mean_feature_importance[1:]:\n",
        "    all_Mean_feature_importance = pd.merge(\n",
        "        all_Mean_feature_importance,\n",
        "        df_fi,\n",
        "        on=\"Feature\",\n",
        "        how=\"inner\",\n",
        "        suffixes=(\"\", f\"_Model_{i+1}\")  # Adding suffixes to avoid column duplication\n",
        "    )\n",
        "\n",
        "# Displaying the final result\n",
        "print(all_Mean_feature_importance)"
      ],
      "metadata": {
        "id": "FhQ5jrYu91zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# FINAL SUMMARY OF RESULTS\n",
        "# ===============================================================\n",
        "\n",
        "# Create DataFrame with model metrics\n",
        "print(\"\\nSummary of Metrics for All Models:\")\n",
        "final_metrics_df = pd.DataFrame(final_metrics)\n",
        "print(tabulate(final_metrics_df, headers='keys', tablefmt='pretty'))\n",
        "\n",
        "# Create the final DataFrame with IDs and observed values\n",
        "all_observed_predicted = pd.DataFrame({'ID_Unico': DB['ID_Unico'], 'atr_content': y})\n",
        "\n",
        "# Add predictions from each model to the final DataFrame\n",
        "for model_num, preds in enumerate(final_predictions, 1):\n",
        "    all_observed_predicted[f'Prediction_Model_{model_num}'] = preds\n",
        "\n",
        "# Calculate the mean of predictions\n",
        "all_observed_predicted['mean_pred'] = all_observed_predicted.filter(like='Prediction_Model_').mean(axis=1)\n",
        "\n",
        "# Calculate the standard deviation of predictions\n",
        "all_observed_predicted['Std_Dev_Prediction'] = all_observed_predicted.filter(like='Prediction_Model_').std(axis=1)\n",
        "\n",
        "# Display the final DataFrame with new columns\n",
        "print(\"\\nFinal DataFrame with Predictions, Mean, and Standard Deviation:\")\n",
        "print(all_observed_predicted)"
      ],
      "metadata": {
        "id": "rSNYnGLk94Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the working directory\n",
        "os.chdir(\"D:/alunos/Bruno_B/final_application/multiespectral_data_approach/Step_6_SySI_data/Carbon\")\n",
        "\n",
        "# Save the final_metrics_df DataFrame as a CSV file\n",
        "final_metrics_df.to_csv(\"acuracia_DB_protocol_multi_filtered_step_5_Carbon.csv\", index=False)\n",
        "\n",
        "# Save all_observed_predicted DataFrame as a CSV file\n",
        "all_observed_predicted.to_csv(\"predictions_DB_protocol_multi_filtered_step_5_Carbon.csv\", index=False)\n",
        "\n",
        "# Save all_Mean_feature_importance DataFrame as a CSV file\n",
        "all_Mean_feature_importance.to_csv(\"ImpCoV_DB_protocol_multi_filtered_step_5_Carbon.csv\", index=False)"
      ],
      "metadata": {
        "id": "uo5YdJrf96Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cutting line determination"
      ],
      "metadata": {
        "id": "FthgZ2AIQD2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from plotnine import *\n",
        "import scipy.stats as stats\n",
        "from plotnine import ggplot"
      ],
      "metadata": {
        "id": "R3eUv4f2QIEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the working directory\n",
        "os.chdir(\"D:/alunos/Bruno_B/final_application/multiespectral_data_approach/Step_6_SySI_data/Carbon\")\n",
        "\n",
        "# Read the CSV file\n",
        "stat_p = pd.read_csv(\"predictions_DB_protocol_multi_filtered_step_5_Carbon.csv\")\n",
        "\n",
        "print(stat_p)"
      ],
      "metadata": {
        "id": "2st4ZktlQTYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cutting line determination ###\n",
        "\n",
        "# Specify the X and Y columns\n",
        "column_x = stat_p['atr_content']\n",
        "column_y = stat_p['mean_pred']\n",
        "\n",
        "# Create a linear regression model\n",
        "X = sm.add_constant(column_x)  # Add the constant for the intercept term\n",
        "model = sm.OLS(column_y, X).fit()\n",
        "\n",
        "# Calculate predicted values\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Calculate the standard deviation of residuals\n",
        "residuals = model.resid\n",
        "std_residuals = np.std(residuals)\n",
        "\n",
        "# Confidence level (e.g., 98%)\n",
        "confidence_level = 0.99\n",
        "\n",
        "# Quantile of the t distribution\n",
        "quantile_t = stats.t.ppf((1 + confidence_level) / 2, df=len(residuals) - 2)\n",
        "\n",
        "# Calculate upper and lower confidence limits\n",
        "upper_limit_attribute = predictions + quantile_t * (std_residuals * 1.15)\n",
        "lower_limit_attribute = predictions - quantile_t * (std_residuals * 1.15)\n",
        "\n",
        "# Regression coefficients\n",
        "intercept, coef_x = model.params\n",
        "\n",
        "# Standard error of coefficients\n",
        "std_error = model.bse\n",
        "\n",
        "# T-value and p-value of coefficients\n",
        "t_value = model.tvalues\n",
        "p_value = model.pvalues\n",
        "\n",
        "# Coefficient of determination (R²)\n",
        "r_squared_simulated = model.rsquared\n",
        "\n",
        "# Fit a linear regression model for the upper limit\n",
        "model_upper_limit = sm.OLS(upper_limit_attribute, X).fit()\n",
        "\n",
        "# Fit a linear regression model for the lower limit\n",
        "model_lower_limit = sm.OLS(lower_limit_attribute, X).fit()\n",
        "\n",
        "# Coefficients of intercept and slope for the upper limit\n",
        "coef_intercept_upper, coef_slope_upper = model_upper_limit.params\n",
        "\n",
        "# Coefficients of intercept and slope for the lower limit\n",
        "coef_intercept_lower, coef_slope_lower = model_lower_limit.params\n",
        "\n",
        "# Display coefficients\n",
        "print(\"Coefficients for Upper Limit:\")\n",
        "print(\"Intercept (β0):\", round(coef_intercept_upper, 4))\n",
        "print(\"Slope (β1):\", round(coef_slope_upper, 4))\n",
        "\n",
        "print(\"\\nCoefficients for Lower Limit:\")\n",
        "print(\"Intercept (β0):\", round(coef_intercept_lower, 4))\n",
        "print(\"Slope (β1):\", round(coef_slope_lower, 4))"
      ],
      "metadata": {
        "id": "KY6EYwTDQos7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Observed clay vs. predicted clay scatterplot ###\n",
        "\n",
        "# Desired width and height of the plot\n",
        "plot_width = 9\n",
        "plot_height = 6\n",
        "\n",
        "scatter_plot = (\n",
        "    ggplot(stat_p, aes(x='atr_content', y='mean_pred')) +\n",
        "    theme_classic() +\n",
        "    geom_point(shape='o', color= 'black', fill= '#8069C7', alpha=1, size=1.5, stroke=0.2) + # Data points with black contour +  # Data points\n",
        "    geom_smooth(method='lm', se=False, color='#000000', size=1.2) +  # Trend line\n",
        "    geom_line(aes(y=upper_limit_attribute), color='#E41A1C', size=1) +  # Upper confidence interval line\n",
        "    geom_line(aes(y=lower_limit_attribute), color='#E41A1C', size=1) +  # Lower confidence interval line\n",
        "\n",
        "    annotate('text', x=200, y=850, label=\"β upper limit: {:.3f}\".format(round(coef_intercept_upper, 3)),\n",
        "             family=\"Arial\", size=9)+\n",
        "    annotate('text', x=200, y=817, label=\"β lower limit: {:.3f}\".format(round(coef_intercept_lower, 3)),\n",
        "             family=\"Arial\", size=9)+\n",
        "    annotate('text', x=200, y=784, label=\"R² model: {:.3f}\".format(round(r_squared_simulated, 3)),\n",
        "             family=\"Arial\", size=9)+\n",
        "    annotate('text', x=200, y=751, label=\"α: {:.3f}\".format(round(coef_x, 3)),\n",
        "             family=\"Arial\", size=9)+\n",
        "\n",
        "    #labs(x=\"Observed clay content (g kg⁻¹)\", y=\"Predicted clay content (g kg⁻¹)\") +\n",
        "    labs(x=\"Observed Clay content (g kg$^{-1}$)\", y=\"Predicted Clay by SySI content (g kg$^{-1}$)\") +\n",
        "    expand_limits(x=0, y=0) +  # Force the plot to show the origin (0,0)\n",
        "    scale_y_continuous(limits=[0, 1000]) +\n",
        "    scale_x_continuous(limits=[0, 1000]) +\n",
        "    theme(\n",
        "        axis_title_x=element_text(margin={'t': 12}, family=\"Arial\", size=14, face=\"bold\"),\n",
        "        axis_title_y=element_text(margin={'r': 12}, family=\"Arial\", size=14, face=\"bold\"),\n",
        "        axis_text_y=element_text(family=\"Arial\", size=12),\n",
        "        axis_text_x=element_text(family=\"Arial\", size=12)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the plot\n",
        "print(scatter_plot)\n",
        "\n",
        "dpi_value = 1000\n",
        "\n",
        "# Setting the working directory\n",
        "output_path = \"D:/alunos/Bruno_B/final_application/multiespectral_data_approach/Step_6_SySI_data/Clay/scatter_plot_DB_protocol_Clay_multi.png\"\n",
        "scatter_plot.save(output_path, width=plot_width, height=plot_height, dpi=dpi_value)\n",
        "\n",
        "scatter_plot"
      ],
      "metadata": {
        "id": "rulLkwv6QvGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Observed C vs. predicted C scatterplot ###\n",
        "\n",
        "# Desired width and height of the plot\n",
        "plot_width = 9\n",
        "plot_height = 6\n",
        "\n",
        "scatter_plot = (\n",
        "    ggplot(stat_p, aes(x='atr_content', y='mean_pred')) +\n",
        "    theme_classic() +\n",
        "    geom_point(shape='o', color= 'black', fill= '#8069C7', alpha=1, size=1.5, stroke=0.2) + # Data points with black contour +  # Data points\n",
        "    geom_smooth(method='lm', se=False, color='#000000', size=1.2) +  # Trend line\n",
        "    geom_line(aes(y=upper_limit_attribute), color='#E41A1C', size=1) +  # Upper confidence interval line\n",
        "    geom_line(aes(y=lower_limit_attribute), color='#E41A1C', size=1) +  # Lower confidence interval line\n",
        "\n",
        "    annotate('text', x=70, y=230, label=\"β upper limit: {:.3f}\".format(round(coef_intercept_upper, 3)),\n",
        "             family=\"Arial\", size=9)+\n",
        "    annotate('text', x=70, y=220, label=\"β lower limit: {:.3f}\".format(round(coef_intercept_lower, 3)),\n",
        "             family=\"Arial\", size=9)+\n",
        "    annotate('text', x=70, y=210, label=\"R² model: {:.3f}\".format(round(r_squared_simulated, 3)),\n",
        "             family=\"Arial\", size=9)+\n",
        "    annotate('text', x=70, y=200, label=\"α: {:.3f}\".format(round(coef_x, 3)),\n",
        "             family=\"Arial\", size=9)+\n",
        "\n",
        "    #labs(x=\"Observed clay content (g kg⁻¹)\", y=\"Predicted clay content (g kg⁻¹)\") +\n",
        "    labs(x=\"Observed C content (g kg$^{-1}$)\", y=\"Predicted C by SySI content (g kg$^{-1}$)\") +\n",
        "    expand_limits(x=0, y=0) +  # Force the plot to show the origin (0,0)\n",
        "    scale_y_continuous(limits=[0, 250]) +\n",
        "    scale_x_continuous(limits=[0, 250]) +\n",
        "    theme(\n",
        "        axis_title_x=element_text(margin={'t': 12}, family=\"Arial\", size=14, face=\"bold\"),\n",
        "        axis_title_y=element_text(margin={'r': 12}, family=\"Arial\", size=14, face=\"bold\"),\n",
        "        axis_text_y=element_text(family=\"Arial\", size=12),\n",
        "        axis_text_x=element_text(family=\"Arial\", size=12)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the plot\n",
        "print(scatter_plot)\n",
        "\n",
        "dpi_value = 1000\n",
        "\n",
        "# Setting the working directory\n",
        "output_path = \"D:/alunos/Bruno_B/final_application/multiespectral_data_approach/Step_6_SySI_data/Carbon/scatter_plot_DB_protocol_Carbon_multi.png\"\n",
        "scatter_plot.save(output_path, width=plot_width, height=plot_height, dpi=dpi_value)\n",
        "\n",
        "scatter_plot"
      ],
      "metadata": {
        "id": "pWpqVyLe-IhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### After the agreement analysis, filtering the samples in the database ###\n",
        "\n",
        "# Create a subset of samples that are ABOVE the upper_limit_attribute line\n",
        "samples_upper_limit = stat_p[stat_p['mean_pred'] > upper_limit_attribute]\n",
        "\n",
        "# Create a subset of samples that are BELOW the lower_limit_attribute line\n",
        "samples_lower_limit = stat_p[stat_p['mean_pred'] < lower_limit_attribute]\n",
        "\n",
        "# Concatenate the dataframes samples_upper_limit and samples_lower_limit\n",
        "outliers_samples = pd.concat([samples_upper_limit, samples_lower_limit])\n",
        "\n",
        "# Merge the DataFrames based on the ID_Unique column and keep only the columns from the DB DataFrame\n",
        "outliers_samples_6 = pd.merge(outliers_samples[['ID_Unico']], DB, on='ID_Unico', how='inner')\n",
        "\n",
        "IDs_to_filter = outliers_samples['ID_Unico'].tolist()\n",
        "\n",
        "DB_filter_step_6 = DB_filter_step_5\n",
        "\n",
        "# Add a new column \"outliers_step_4\" filled with False\n",
        "DB_filter_step_6.insert(1, \"outliers_step_6\", False)\n",
        "\n",
        "# Update the values to True where ID_Unique is in the list IDs_to_filter\n",
        "DB_filter_step_6.loc[DB_filter_step_6['ID_Unico'].isin(IDs_to_filter), 'outliers_step_6'] = True\n",
        "\n",
        "# Create a new DataFrame with only the rows where outliers_step_4 is True\n",
        "outliers_samples_step_6 = DB_filter_step_6[DB_filter_step_6['outliers_step_6'] == True].copy()\n",
        "\n",
        "# Remove the samples where outliers_step_6 is True\n",
        "DB_filter_step_6 = DB_filter_step_6[DB_filter_step_6['outliers_step_6'] == False].copy()\n",
        "\n",
        "print(outliers_samples_step_6)\n"
      ],
      "metadata": {
        "id": "bG7czURaQ1_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of samples in the initial dataframe of the attribute that was analyzed,\n",
        "# outliers outliers_samples_step_6, and DB_filter_step_6 dataframe\n",
        "\n",
        "# Print the number of samples in DB_filter_step_5\n",
        "print(\"Number of samples in DB_filter_step_5:\", DB_filter_step_5.shape[0])\n",
        "\n",
        "# Print the number of samples in outliers_samples_step_6\n",
        "print(\"Number of samples in outliers_samples_step_6:\", outliers_samples_step_6.shape[0])\n",
        "\n",
        "# Print the number of samples in DB_filter_step_6\n",
        "print(\"Number of samples in DB_filter_step_6:\", DB_filter_step_6.shape[0])"
      ],
      "metadata": {
        "id": "my1j0iONRPwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DB_filter_step_6 and outliers_samples_step_6 DataFrame as a CSV file\n",
        "# Setting the working directory\n",
        "os.chdir(\"D:/alunos/Bruno_B/final_application/multiespectral_data_approach/Step_6_SySI_data/Carbon\")\n",
        "\n",
        "# Save the outliers_samples_6 DataFrame as a CSV file\n",
        "outliers_samples_step_6.to_csv(\"outliers_samples_multispectraldata_step_6_Carbon.csv\", index=False)\n",
        "\n",
        "# Save the DB_filter_6 DataFrame as a CSV file\n",
        "DB_filter_step_6.to_csv(\"DB_protocol_multi_filter_step_6_Carbon_1.15.csv\", index=False)"
      ],
      "metadata": {
        "id": "jwDda-nnRTcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clear all variables**"
      ],
      "metadata": {
        "id": "X2yf1A-ZswIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove todas as variáveis do ambiente\n",
        "%reset -f\n",
        "\n",
        "# Remove todas as imports e módulos carregados\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "4YfEjB8Ms78x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}